c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime

import matplotlib.pyplot as plt
import torch
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import *

# from src.OTFlowProblem import *
# from src.birth_death_process import *
from source_without_BD import *

import config

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 100
    def_batch    = 4096
    def_niter    = 10000
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['1d','swissroll','2gaussians','8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='moons'
)

parser.add_argument("--nt"    , type=int, default=8, help="number of time steps") # origin:8
parser.add_argument("--nt_val", type=int, default=8, help="number of time steps for validation") # origin:8
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0') # origin 1, 100, 5
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)

parser.add_argument('--lr'          , type=float, default=0.05)
parser.add_argument("--drop_freq"   , type=int  , default=200, help="how often to decrease learning rate") # origin 100
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='double', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=3)
parser.add_argument('--sample_freq', type=int, default=25)
parser.add_argument('--logrho0_freq', type=int, default=1000)
parser.add_argument('--alphaa', type=float, default=10)

args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]


# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

def compute_loss(net, x,x2, rho_x, nt):


    costC1,costL1, costV1, cs, weights, position = OTFlowProblem_ex(x, net, [0, 1], nt=nt, stepper="rk4",
                                                                    alph=net.alph, is_1d=False, alphaa=args.alphaa,jjj=0,device=device)


    # costC2, costL2, costV2, cs2, weights2, position2 = OTFlowProblem_ex(x2, net, [1, 0], nt=nt, stepper="rk4",
    #                                                                     alph=net.alph, is_1d=False, alphaa=args.alphaa,
    #                                                                     jjj=1, device=device)

    #
    # position4 = position2.cpu().detach().numpy()
    # plt.plot(position4[:, 0], position4[:, 1])
    # plt.show()
    # d=2
    # genModel, _ = integrate_ex(x2[:, 0:d], net, [1, 0.0], nt_val, stepper="rk4", alph=net.alph)
    # costC2 = genModel[0, -4] / x.shape[0]

    # a = torch.sum(genModel[:,-1])
    # b = genModel[:,-1]
    # costC2 = torch.mean(torch.log(genModel[:,-1]))
    # Jc, cs = OTFlowProblem(x, net, [0, 1], nt=nt, stepper="rk4", alph=net.alph, is_1d=True)

    # costL1 = torch.sum(genModel[:,-3])
    # costV1 = torch.sum(genModel[:,-5])
    # print(costC2+costC1, costL1, costV1)
    # print('weights_min:{},weights_max:{}, weights2:{}, weights2:{}'.format(torch.min(weights),torch.max(weights),torch.min(weights2),torch.max(weights2)))

    Jc = (costC1)*100+costL1*5+costV1*1
    return Jc, cs, weights, position


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None

    # setup data [nSamples, d]
    # use one batch as the entire data set
    print(args.data)
    x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=True,device=device)
    x0 = cvt(torch.from_numpy(x0))
    rho_x0 = cvt(torch.from_numpy(rho_x0))
    x2 = torch.randn((4096, 2)).to(device)
    x3 = torch.randn((4096, 2)).to(device)

    x0val, rho_x0val, d_net2 = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size,require_density=True,device=device)
    # x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, require_density=True)
    x0val = cvt(torch.from_numpy(x0val))
    rho_x0val = cvt(torch.from_numpy(rho_x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s} {:9s}     {:9s}  {:9s}  {:9s}  {:9s} {:9s} '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'V(velocity)', 'valLoss', 'valL', 'valC', 'valR',
            'valV'
        )
    )

    logger.info(log_msg)

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs, weights, position = compute_loss(net, x0, x2, rho_x0, nt=args.nt)

        # torch.autograd.set_detect_anomaly(True)
        loss.backward()
        optim.step()

        # position3 = position.cpu().detach().numpy()
        # plt.plot(position3[:, 0], position3[:, 1])
        # plt.show()

        # if itr==1 or itr%100==0:
        #     a = torch.linspace(-4, 4, 1000)
        #     uu = []
        #     for i in range(1000):
        #         uu.append(torch.cat((a[i] * torch.ones_like(a).unsqueeze(-1), a.unsqueeze(-1)), dim=-1))
        #     u = torch.cat(uu).to(device)
        #     t = 0
        #     for i in range(9):
        #         sPath2 = os.path.join(args.save, 'figs2', start_time + '_{}.png'.format(t))
        #         if not os.path.exists(os.path.dirname(sPath2)):
        #             os.makedirs(os.path.dirname(sPath2))
        #         z = pad(u, (0, 1, 0, 0), value=t)
        #         q = net(z).reshape(1000, 1000)
        #         figure = plt.figure()
        #         axes = Axes3D(figure)
        #         XX = np.linspace(-4, 4, 1000)
        #         TT = np.linspace(-4,4,1000)
        #         XX, TT = np.meshgrid(XX, TT)
        #         plt.xlabel("X")
        #         plt.ylabel("T")
        #         plt.title("itr:{}, t:{}".format(0, t))
        #         q = q.cpu().detach().numpy()
        #         axes.plot_surface(XX, TT, q, cmap='rainbow')
        #         plt.savefig(sPath2, dpi=300)
        #         t = t + 0.125

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate
        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs, test_weights, test_position = compute_loss(net, x0val, x3, rho_x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()

        logger.info(log_message)

        # if itr % args.logrho0_freq == 0:
        #     x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, require_density=True, device=device)
        #     x0 = cvt(torch.from_numpy(x0))
        #     rho_x0 = cvt(torch.from_numpy(rho_x0))

        # create plots
        if itr % args.viz_freq == 0:
            nSamples = 20000
            p_samples = toy_data.inf_train_gen(args.data, batch_size=nSamples, require_density=False,device=device)
            p_samples = cvt(torch.from_numpy(p_samples))
            y = cvt(torch.randn(nSamples, d))
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                # plot1d_BD(net, p_samples, y, nt_val, sPath, doPaths=False, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                #             ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh),alphaa=args.alphaa)
                plot2d(net, p_samples, y, nt_val, sPath, doPaths=True,
                          sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                                 ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1],
                                                                        alph[2], nt, m, nTh))
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=False,device=device)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu
            y0 = d_net(x0)
            y0 = y0.detach().cpu().numpy()
            x1 = x0.detach().cpu().numpy()
            w = np.sum(np.power(x1, 2), 1, keepdims=True)
            # rho_x0 = np.exp(-w/2+y0)/((2*pi)**(d/2))
            # rho_x0 = cvt(torch.from_numpy(rho_x0))
            x2 = torch.randn((10000, 2)).to(device)

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
start time: 2022_03_21_16_03_01
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=3, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime

import matplotlib.pyplot as plt
import torch
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import *

# from src.OTFlowProblem import *
# from src.birth_death_process import *
from source_without_BD import *

import config

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 100
    def_batch    = 4096
    def_niter    = 10000
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['1d','swissroll','2gaussians','8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='moons'
)

parser.add_argument("--nt"    , type=int, default=8, help="number of time steps") # origin:8
parser.add_argument("--nt_val", type=int, default=8, help="number of time steps for validation") # origin:8
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0') # origin 1, 100, 5
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)

parser.add_argument('--lr'          , type=float, default=0.05)
parser.add_argument("--drop_freq"   , type=int  , default=200, help="how often to decrease learning rate") # origin 100
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='double', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=0)
parser.add_argument('--sample_freq', type=int, default=25)
parser.add_argument('--logrho0_freq', type=int, default=1000)
parser.add_argument('--alphaa', type=float, default=10)

args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]


# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

def compute_loss(net, x,x2, rho_x, nt):


    costC1,costL1, costV1, cs, weights, position = OTFlowProblem_ex(x, net, [0, 1], nt=nt, stepper="rk4",
                                                                    alph=net.alph, is_1d=False, alphaa=args.alphaa,jjj=0,device=device)


    # costC2, costL2, costV2, cs2, weights2, position2 = OTFlowProblem_ex(x2, net, [1, 0], nt=nt, stepper="rk4",
    #                                                                     alph=net.alph, is_1d=False, alphaa=args.alphaa,
    #                                                                     jjj=1, device=device)

    #
    # position4 = position2.cpu().detach().numpy()
    # plt.plot(position4[:, 0], position4[:, 1])
    # plt.show()
    # d=2
    # genModel, _ = integrate_ex(x2[:, 0:d], net, [1, 0.0], nt_val, stepper="rk4", alph=net.alph)
    # costC2 = genModel[0, -4] / x.shape[0]

    # a = torch.sum(genModel[:,-1])
    # b = genModel[:,-1]
    # costC2 = torch.mean(torch.log(genModel[:,-1]))
    # Jc, cs = OTFlowProblem(x, net, [0, 1], nt=nt, stepper="rk4", alph=net.alph, is_1d=True)

    # costL1 = torch.sum(genModel[:,-3])
    # costV1 = torch.sum(genModel[:,-5])
    # print(costC2+costC1, costL1, costV1)
    # print('weights_min:{},weights_max:{}, weights2:{}, weights2:{}'.format(torch.min(weights),torch.max(weights),torch.min(weights2),torch.max(weights2)))

    Jc = (costC1)*100+costL1*5+costV1*1
    return Jc, cs, weights, position


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None

    # setup data [nSamples, d]
    # use one batch as the entire data set
    print(args.data)
    x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=True,device=device)
    x0 = cvt(torch.from_numpy(x0))
    rho_x0 = cvt(torch.from_numpy(rho_x0))
    x2 = torch.randn((4096, 2)).to(device)
    x3 = torch.randn((4096, 2)).to(device)

    x0val, rho_x0val, d_net2 = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size,require_density=True,device=device)
    # x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, require_density=True)
    x0val = cvt(torch.from_numpy(x0val))
    rho_x0val = cvt(torch.from_numpy(rho_x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s} {:9s}     {:9s}  {:9s}  {:9s}  {:9s} {:9s} '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'V(velocity)', 'valLoss', 'valL', 'valC', 'valR',
            'valV'
        )
    )

    logger.info(log_msg)

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs, weights, position = compute_loss(net, x0, x2, rho_x0, nt=args.nt)

        # torch.autograd.set_detect_anomaly(True)
        loss.backward()
        optim.step()

        # position3 = position.cpu().detach().numpy()
        # plt.plot(position3[:, 0], position3[:, 1])
        # plt.show()

        # if itr==1 or itr%100==0:
        #     a = torch.linspace(-4, 4, 1000)
        #     uu = []
        #     for i in range(1000):
        #         uu.append(torch.cat((a[i] * torch.ones_like(a).unsqueeze(-1), a.unsqueeze(-1)), dim=-1))
        #     u = torch.cat(uu).to(device)
        #     t = 0
        #     for i in range(9):
        #         sPath2 = os.path.join(args.save, 'figs2', start_time + '_{}.png'.format(t))
        #         if not os.path.exists(os.path.dirname(sPath2)):
        #             os.makedirs(os.path.dirname(sPath2))
        #         z = pad(u, (0, 1, 0, 0), value=t)
        #         q = net(z).reshape(1000, 1000)
        #         figure = plt.figure()
        #         axes = Axes3D(figure)
        #         XX = np.linspace(-4, 4, 1000)
        #         TT = np.linspace(-4,4,1000)
        #         XX, TT = np.meshgrid(XX, TT)
        #         plt.xlabel("X")
        #         plt.ylabel("T")
        #         plt.title("itr:{}, t:{}".format(0, t))
        #         q = q.cpu().detach().numpy()
        #         axes.plot_surface(XX, TT, q, cmap='rainbow')
        #         plt.savefig(sPath2, dpi=300)
        #         t = t + 0.125

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate
        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs, test_weights, test_position = compute_loss(net, x0val, x3, rho_x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()

        logger.info(log_message)

        # if itr % args.logrho0_freq == 0:
        #     x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, require_density=True, device=device)
        #     x0 = cvt(torch.from_numpy(x0))
        #     rho_x0 = cvt(torch.from_numpy(rho_x0))

        # create plots
        if itr % args.viz_freq == 0:
            nSamples = 20000
            p_samples = toy_data.inf_train_gen(args.data, batch_size=nSamples, require_density=False,device=device)
            p_samples = cvt(torch.from_numpy(p_samples))
            y = cvt(torch.randn(nSamples, d))
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                # plot1d_BD(net, p_samples, y, nt_val, sPath, doPaths=False, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                #             ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh),alphaa=args.alphaa)
                plot2d(net, p_samples, y, nt_val, sPath, doPaths=True,
                          sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                                 ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1],
                                                                        alph[2], nt, m, nTh))
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=False,device=device)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu
            y0 = d_net(x0)
            y0 = y0.detach().cpu().numpy()
            x1 = x0.detach().cpu().numpy()
            w = np.sum(np.power(x1, 2), 1, keepdims=True)
            # rho_x0 = np.exp(-w/2+y0)/((2*pi)**(d/2))
            # rho_x0 = cvt(torch.from_numpy(rho_x0))
            x2 = torch.randn((10000, 2)).to(device)

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
start time: 2022_03_21_16_03_46
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   2.547   1.245e+03  4.886e+00  1.158e+01  6.244e+01     1.169e+03  4.612e+00  1.087e+01  5.914e+01 
00002   0.155   1.170e+03  4.641e+00  1.087e+01  5.943e+01     7.891e+02  2.913e+00  7.535e+00  2.105e+01 
00003   0.152   7.892e+02  2.931e+00  7.534e+00  2.115e+01     5.595e+02  1.760e+00  5.441e+00  6.527e+00 
00004   0.151   5.596e+02  1.770e+00  5.442e+00  6.558e+00     4.616e+02  1.233e+00  4.527e+00  2.737e+00 
00005   0.157   4.619e+02  1.240e+00  4.529e+00  2.751e+00     4.106e+02  9.535e-01  4.045e+00  1.332e+00 
00006   0.153   4.109e+02  9.592e-01  4.047e+00  1.339e+00     3.813e+02  7.719e-01  3.768e+00  7.013e-01 
00007   0.155   3.817e+02  7.767e-01  3.771e+00  7.051e-01     3.630e+02  5.979e-01  3.596e+00  4.151e-01 
00008   0.149   3.634e+02  6.020e-01  3.600e+00  4.170e-01     3.524e+02  4.389e-01  3.499e+00  2.932e-01 
00009   0.150   3.529e+02  4.421e-01  3.504e+00  2.940e-01     3.482e+02  3.512e-01  3.462e+00  2.565e-01 
00010   0.159   3.487e+02  3.536e-01  3.467e+00  2.569e-01     3.458e+02  3.356e-01  3.438e+00  2.587e-01 
00011   0.149   3.464e+02  3.378e-01  3.444e+00  2.589e-01     3.443e+02  3.673e-01  3.422e+00  2.730e-01 
00012   0.158   3.449e+02  3.696e-01  3.428e+00  2.731e-01     3.434e+02  4.019e-01  3.410e+00  2.953e-01 
00013   0.150   3.439e+02  4.045e-01  3.416e+00  2.954e-01     3.417e+02  4.079e-01  3.393e+00  3.305e-01 
00014   0.164   3.422e+02  4.109e-01  3.398e+00  3.306e-01     3.413e+02  4.179e-01  3.389e+00  3.822e-01 
00015   0.149   3.418e+02  4.211e-01  3.393e+00  3.823e-01     3.416e+02  4.532e-01  3.388e+00  4.438e-01 
00016   0.149   3.420e+02  4.565e-01  3.393e+00  4.440e-01     3.414e+02  5.027e-01  3.384e+00  4.921e-01 
00017   0.148   3.418e+02  5.060e-01  3.388e+00  4.924e-01     3.416e+02  5.409e-01  3.384e+00  4.986e-01 
00018   0.148   3.420e+02  5.443e-01  3.388e+00  4.990e-01     3.407e+02  5.243e-01  3.377e+00  4.605e-01 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime

import matplotlib.pyplot as plt
import torch
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import *

# from src.OTFlowProblem import *
# from src.birth_death_process import *
from source_without_BD import *

import config

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 100
    def_batch    = 4096
    def_niter    = 10000
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['1d','swissroll','2gaussians','8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='moons'
)

parser.add_argument("--nt"    , type=int, default=8, help="number of time steps") # origin:8
parser.add_argument("--nt_val", type=int, default=8, help="number of time steps for validation") # origin:8
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0') # origin 1, 100, 5
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)

parser.add_argument('--lr'          , type=float, default=0.05)
parser.add_argument("--drop_freq"   , type=int  , default=200, help="how often to decrease learning rate") # origin 100
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='double', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=0)
parser.add_argument('--sample_freq', type=int, default=25)
parser.add_argument('--logrho0_freq', type=int, default=1000)
parser.add_argument('--alphaa', type=float, default=10)

args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]


# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

def compute_loss(net, x,x2, rho_x, nt):


    costC1,costL1, costV1, cs, weights, position = OTFlowProblem_ex(x, net, [0, 1], nt=nt, stepper="rk4",
                                                                    alph=net.alph, is_1d=False, alphaa=args.alphaa,jjj=0,device=device)


    # costC2, costL2, costV2, cs2, weights2, position2 = OTFlowProblem_ex(x2, net, [1, 0], nt=nt, stepper="rk4",
    #                                                                     alph=net.alph, is_1d=False, alphaa=args.alphaa,
    #                                                                     jjj=1, device=device)

    #
    # position4 = position2.cpu().detach().numpy()
    # plt.plot(position4[:, 0], position4[:, 1])
    # plt.show()
    # d=2
    # genModel, _ = integrate_ex(x2[:, 0:d], net, [1, 0.0], nt_val, stepper="rk4", alph=net.alph)
    # costC2 = genModel[0, -4] / x.shape[0]

    # a = torch.sum(genModel[:,-1])
    # b = genModel[:,-1]
    # costC2 = torch.mean(torch.log(genModel[:,-1]))
    # Jc, cs = OTFlowProblem(x, net, [0, 1], nt=nt, stepper="rk4", alph=net.alph, is_1d=True)

    # costL1 = torch.sum(genModel[:,-3])
    # costV1 = torch.sum(genModel[:,-5])
    # print(costC2+costC1, costL1, costV1)
    # print('weights_min:{},weights_max:{}, weights2:{}, weights2:{}'.format(torch.min(weights),torch.max(weights),torch.min(weights2),torch.max(weights2)))

    Jc = (costC1)*100+costL1*5+costV1*1
    return Jc, cs, weights, position


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None

    # setup data [nSamples, d]
    # use one batch as the entire data set
    print(args.data)
    x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=True,device=device)
    x0 = cvt(torch.from_numpy(x0))
    rho_x0 = cvt(torch.from_numpy(rho_x0))
    x2 = torch.randn((4096, 2)).to(device)
    x3 = torch.randn((4096, 2)).to(device)

    x0val, rho_x0val, d_net2 = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size,require_density=True,device=device)
    # x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, require_density=True)
    x0val = cvt(torch.from_numpy(x0val))
    rho_x0val = cvt(torch.from_numpy(rho_x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s} {:9s}     {:9s}  {:9s}  {:9s}  {:9s} {:9s} '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'V(velocity)', 'valLoss', 'valL', 'valC', 'valR',
            'valV'
        )
    )

    logger.info(log_msg)

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs, weights, position = compute_loss(net, x0, x2, rho_x0, nt=args.nt)

        # torch.autograd.set_detect_anomaly(True)
        loss.backward()
        optim.step()

        # position3 = position.cpu().detach().numpy()
        # plt.plot(position3[:, 0], position3[:, 1])
        # plt.show()

        # if itr==1 or itr%100==0:
        #     a = torch.linspace(-4, 4, 1000)
        #     uu = []
        #     for i in range(1000):
        #         uu.append(torch.cat((a[i] * torch.ones_like(a).unsqueeze(-1), a.unsqueeze(-1)), dim=-1))
        #     u = torch.cat(uu).to(device)
        #     t = 0
        #     for i in range(9):
        #         sPath2 = os.path.join(args.save, 'figs2', start_time + '_{}.png'.format(t))
        #         if not os.path.exists(os.path.dirname(sPath2)):
        #             os.makedirs(os.path.dirname(sPath2))
        #         z = pad(u, (0, 1, 0, 0), value=t)
        #         q = net(z).reshape(1000, 1000)
        #         figure = plt.figure()
        #         axes = Axes3D(figure)
        #         XX = np.linspace(-4, 4, 1000)
        #         TT = np.linspace(-4,4,1000)
        #         XX, TT = np.meshgrid(XX, TT)
        #         plt.xlabel("X")
        #         plt.ylabel("T")
        #         plt.title("itr:{}, t:{}".format(0, t))
        #         q = q.cpu().detach().numpy()
        #         axes.plot_surface(XX, TT, q, cmap='rainbow')
        #         plt.savefig(sPath2, dpi=300)
        #         t = t + 0.125

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate
        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs, test_weights, test_position = compute_loss(net, x0val, x3, rho_x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()

        logger.info(log_message)

        # if itr % args.logrho0_freq == 0:
        #     x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, require_density=True, device=device)
        #     x0 = cvt(torch.from_numpy(x0))
        #     rho_x0 = cvt(torch.from_numpy(rho_x0))

        # create plots
        if itr % args.viz_freq == 0:
            nSamples = 20000
            p_samples = toy_data.inf_train_gen(args.data, batch_size=nSamples, require_density=False,device=device)
            p_samples = cvt(torch.from_numpy(p_samples))
            y = cvt(torch.randn(nSamples, d))
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                # plot1d_BD(net, p_samples, y, nt_val, sPath, doPaths=False, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                #             ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh),alphaa=args.alphaa)
                plot2d(net, p_samples, y, nt_val, sPath, doPaths=True,
                          sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                                 ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1],
                                                                        alph[2], nt, m, nTh))
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=False,device=device)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu
            y0 = d_net(x0)
            y0 = y0.detach().cpu().numpy()
            x1 = x0.detach().cpu().numpy()
            w = np.sum(np.power(x1, 2), 1, keepdims=True)
            # rho_x0 = np.exp(-w/2+y0)/((2*pi)**(d/2))
            # rho_x0 = cvt(torch.from_numpy(rho_x0))
            x2 = torch.randn((10000, 2)).to(device)

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
start time: 2022_03_21_16_07_26
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.560   1.262e+03  5.052e+00  1.171e+01  6.610e+01     1.256e+03  4.943e+00  1.162e+01  6.859e+01 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime

import matplotlib.pyplot as plt
import torch
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import *

# from src.OTFlowProblem import *
# from src.birth_death_process import *
from source_without_BD import *

import config

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 100
    def_batch    = 4096
    def_niter    = 10000
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['1d','swissroll','2gaussians','8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='moons'
)

parser.add_argument("--nt"    , type=int, default=8, help="number of time steps") # origin:8
parser.add_argument("--nt_val", type=int, default=8, help="number of time steps for validation") # origin:8
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0') # origin 1, 100, 5
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)

parser.add_argument('--lr'          , type=float, default=0.05)
parser.add_argument("--drop_freq"   , type=int  , default=200, help="how often to decrease learning rate") # origin 100
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='double', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=0)
parser.add_argument('--sample_freq', type=int, default=25)
parser.add_argument('--logrho0_freq', type=int, default=1000)
parser.add_argument('--alphaa', type=float, default=10)

args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]


# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

def compute_loss(net, x,x2, rho_x, nt):


    costC1,costL1, costV1, cs, weights, position = OTFlowProblem_ex(x, net, [0, 1], nt=nt, stepper="rk4",
                                                                    alph=net.alph, is_1d=False, alphaa=args.alphaa,jjj=0,device=device)


    # costC2, costL2, costV2, cs2, weights2, position2 = OTFlowProblem_ex(x2, net, [1, 0], nt=nt, stepper="rk4",
    #                                                                     alph=net.alph, is_1d=False, alphaa=args.alphaa,
    #                                                                     jjj=1, device=device)

    #
    # position4 = position2.cpu().detach().numpy()
    # plt.plot(position4[:, 0], position4[:, 1])
    # plt.show()
    # d=2
    # genModel, _ = integrate_ex(x2[:, 0:d], net, [1, 0.0], nt_val, stepper="rk4", alph=net.alph)
    # costC2 = genModel[0, -4] / x.shape[0]

    # a = torch.sum(genModel[:,-1])
    # b = genModel[:,-1]
    # costC2 = torch.mean(torch.log(genModel[:,-1]))
    # Jc, cs = OTFlowProblem(x, net, [0, 1], nt=nt, stepper="rk4", alph=net.alph, is_1d=True)

    # costL1 = torch.sum(genModel[:,-3])
    # costV1 = torch.sum(genModel[:,-5])
    # print(costC2+costC1, costL1, costV1)
    # print('weights_min:{},weights_max:{}, weights2:{}, weights2:{}'.format(torch.min(weights),torch.max(weights),torch.min(weights2),torch.max(weights2)))

    Jc = (costC1)*100+costL1*5+costV1*1
    return Jc, cs, weights, position


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None

    # setup data [nSamples, d]
    # use one batch as the entire data set
    print(args.data)
    x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=True,device=device)
    x0 = cvt(torch.from_numpy(x0))
    rho_x0 = cvt(torch.from_numpy(rho_x0))
    x2 = torch.randn((4096, 2)).to(device)
    x3 = torch.randn((4096, 2)).to(device)

    x0val, rho_x0val, d_net2 = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size,require_density=True,device=device)
    # x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, require_density=True)
    x0val = cvt(torch.from_numpy(x0val))
    rho_x0val = cvt(torch.from_numpy(rho_x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s} {:9s}     {:9s}  {:9s}  {:9s}  {:9s} {:9s} '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'V(velocity)', 'valLoss', 'valL', 'valC', 'valR',
            'valV'
        )
    )
    print("---------------------------------------------------")
    logger.info(log_msg)
    print("---------------------------------------------------")

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs, weights, position = compute_loss(net, x0, x2, rho_x0, nt=args.nt)

        # torch.autograd.set_detect_anomaly(True)
        loss.backward()
        optim.step()

        # position3 = position.cpu().detach().numpy()
        # plt.plot(position3[:, 0], position3[:, 1])
        # plt.show()

        # if itr==1 or itr%100==0:
        #     a = torch.linspace(-4, 4, 1000)
        #     uu = []
        #     for i in range(1000):
        #         uu.append(torch.cat((a[i] * torch.ones_like(a).unsqueeze(-1), a.unsqueeze(-1)), dim=-1))
        #     u = torch.cat(uu).to(device)
        #     t = 0
        #     for i in range(9):
        #         sPath2 = os.path.join(args.save, 'figs2', start_time + '_{}.png'.format(t))
        #         if not os.path.exists(os.path.dirname(sPath2)):
        #             os.makedirs(os.path.dirname(sPath2))
        #         z = pad(u, (0, 1, 0, 0), value=t)
        #         q = net(z).reshape(1000, 1000)
        #         figure = plt.figure()
        #         axes = Axes3D(figure)
        #         XX = np.linspace(-4, 4, 1000)
        #         TT = np.linspace(-4,4,1000)
        #         XX, TT = np.meshgrid(XX, TT)
        #         plt.xlabel("X")
        #         plt.ylabel("T")
        #         plt.title("itr:{}, t:{}".format(0, t))
        #         q = q.cpu().detach().numpy()
        #         axes.plot_surface(XX, TT, q, cmap='rainbow')
        #         plt.savefig(sPath2, dpi=300)
        #         t = t + 0.125

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate
        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs, test_weights, test_position = compute_loss(net, x0val, x3, rho_x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()

        logger.info(log_message)

        # if itr % args.logrho0_freq == 0:
        #     x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, require_density=True, device=device)
        #     x0 = cvt(torch.from_numpy(x0))
        #     rho_x0 = cvt(torch.from_numpy(rho_x0))

        # create plots
        if itr % args.viz_freq == 0:
            nSamples = 20000
            p_samples = toy_data.inf_train_gen(args.data, batch_size=nSamples, require_density=False,device=device)
            p_samples = cvt(torch.from_numpy(p_samples))
            y = cvt(torch.randn(nSamples, d))
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                # plot1d_BD(net, p_samples, y, nt_val, sPath, doPaths=False, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                #             ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh),alphaa=args.alphaa)
                plot2d(net, p_samples, y, nt_val, sPath, doPaths=True,
                          sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                                 ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1],
                                                                        alph[2], nt, m, nTh))
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=False,device=device)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu
            y0 = d_net(x0)
            y0 = y0.detach().cpu().numpy()
            x1 = x0.detach().cpu().numpy()
            w = np.sum(np.power(x1, 2), 1, keepdims=True)
            # rho_x0 = np.exp(-w/2+y0)/((2*pi)**(d/2))
            # rho_x0 = cvt(torch.from_numpy(rho_x0))
            x2 = torch.randn((10000, 2)).to(device)

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
start time: 2022_03_22_14_07_09
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime

import matplotlib.pyplot as plt
import torch
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import *

# from src.OTFlowProblem import *
# from src.birth_death_process import *
from source_without_BD import *

import config

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 100
    def_batch    = 4096
    def_niter    = 10000
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['1d','swissroll','2gaussians','8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='moons'
)

parser.add_argument("--nt"    , type=int, default=8, help="number of time steps") # origin:8
parser.add_argument("--nt_val", type=int, default=8, help="number of time steps for validation") # origin:8
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0') # origin 1, 100, 5
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)

parser.add_argument('--lr'          , type=float, default=0.05)
parser.add_argument("--drop_freq"   , type=int  , default=200, help="how often to decrease learning rate") # origin 100
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='double', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=0)
parser.add_argument('--sample_freq', type=int, default=25)
parser.add_argument('--logrho0_freq', type=int, default=1000)
parser.add_argument('--alphaa', type=float, default=10)

args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]


# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

def compute_loss(net, x,x2, rho_x, nt):


    costC1,costL1, costV1, cs, weights, position = OTFlowProblem_ex(x, net, [0, 1], nt=nt, stepper="rk4",
                                                                    alph=net.alph, is_1d=False, alphaa=args.alphaa,jjj=0,device=device)


    # costC2, costL2, costV2, cs2, weights2, position2 = OTFlowProblem_ex(x2, net, [1, 0], nt=nt, stepper="rk4",
    #                                                                     alph=net.alph, is_1d=False, alphaa=args.alphaa,
    #                                                                     jjj=1, device=device)

    #
    # position4 = position2.cpu().detach().numpy()
    # plt.plot(position4[:, 0], position4[:, 1])
    # plt.show()
    # d=2
    # genModel, _ = integrate_ex(x2[:, 0:d], net, [1, 0.0], nt_val, stepper="rk4", alph=net.alph)
    # costC2 = genModel[0, -4] / x.shape[0]

    # a = torch.sum(genModel[:,-1])
    # b = genModel[:,-1]
    # costC2 = torch.mean(torch.log(genModel[:,-1]))
    # Jc, cs = OTFlowProblem(x, net, [0, 1], nt=nt, stepper="rk4", alph=net.alph, is_1d=True)

    # costL1 = torch.sum(genModel[:,-3])
    # costV1 = torch.sum(genModel[:,-5])
    # print(costC2+costC1, costL1, costV1)
    # print('weights_min:{},weights_max:{}, weights2:{}, weights2:{}'.format(torch.min(weights),torch.max(weights),torch.min(weights2),torch.max(weights2)))

    Jc = (costC1)*100+costL1*5+costV1*1
    return Jc, cs, weights, position


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None

    # setup data [nSamples, d]
    # use one batch as the entire data set
    print(args.data)
    x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=True,device=device)
    x0 = cvt(torch.from_numpy(x0))
    rho_x0 = cvt(torch.from_numpy(rho_x0))
    x2 = torch.randn((4096, 2)).to(device)
    x3 = torch.randn((4096, 2)).to(device)

    x0val, rho_x0val, d_net2 = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size,require_density=True,device=device)
    # x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, require_density=True)
    x0val = cvt(torch.from_numpy(x0val))
    rho_x0val = cvt(torch.from_numpy(rho_x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s} {:9s}     {:9s}  {:9s}  {:9s}  {:9s} {:9s} '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'V(velocity)', 'valLoss', 'valL', 'valC', 'valR',
            'valV'
        )
    )
    print("---------------------------------------------------11111")
    logger.info(log_msg)
    print("---------------------------------------------------222222")

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs, weights, position = compute_loss(net, x0, x2, rho_x0, nt=args.nt)

        # torch.autograd.set_detect_anomaly(True)
        loss.backward()
        optim.step()

        # position3 = position.cpu().detach().numpy()
        # plt.plot(position3[:, 0], position3[:, 1])
        # plt.show()

        # if itr==1 or itr%100==0:
        #     a = torch.linspace(-4, 4, 1000)
        #     uu = []
        #     for i in range(1000):
        #         uu.append(torch.cat((a[i] * torch.ones_like(a).unsqueeze(-1), a.unsqueeze(-1)), dim=-1))
        #     u = torch.cat(uu).to(device)
        #     t = 0
        #     for i in range(9):
        #         sPath2 = os.path.join(args.save, 'figs2', start_time + '_{}.png'.format(t))
        #         if not os.path.exists(os.path.dirname(sPath2)):
        #             os.makedirs(os.path.dirname(sPath2))
        #         z = pad(u, (0, 1, 0, 0), value=t)
        #         q = net(z).reshape(1000, 1000)
        #         figure = plt.figure()
        #         axes = Axes3D(figure)
        #         XX = np.linspace(-4, 4, 1000)
        #         TT = np.linspace(-4,4,1000)
        #         XX, TT = np.meshgrid(XX, TT)
        #         plt.xlabel("X")
        #         plt.ylabel("T")
        #         plt.title("itr:{}, t:{}".format(0, t))
        #         q = q.cpu().detach().numpy()
        #         axes.plot_surface(XX, TT, q, cmap='rainbow')
        #         plt.savefig(sPath2, dpi=300)
        #         t = t + 0.125

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate
        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs, test_weights, test_position = compute_loss(net, x0val, x3, rho_x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()

        logger.info(log_message)

        # if itr % args.logrho0_freq == 0:
        #     x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, require_density=True, device=device)
        #     x0 = cvt(torch.from_numpy(x0))
        #     rho_x0 = cvt(torch.from_numpy(rho_x0))

        # create plots
        if itr % args.viz_freq == 0:
            nSamples = 20000
            p_samples = toy_data.inf_train_gen(args.data, batch_size=nSamples, require_density=False,device=device)
            p_samples = cvt(torch.from_numpy(p_samples))
            y = cvt(torch.randn(nSamples, d))
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                # plot1d_BD(net, p_samples, y, nt_val, sPath, doPaths=False, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                #             ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh),alphaa=args.alphaa)
                plot2d(net, p_samples, y, nt_val, sPath, doPaths=True,
                          sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                                 ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1],
                                                                        alph[2], nt, m, nTh))
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=False,device=device)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu
            y0 = d_net(x0)
            y0 = y0.detach().cpu().numpy()
            x1 = x0.detach().cpu().numpy()
            w = np.sum(np.power(x1, 2), 1, keepdims=True)
            # rho_x0 = np.exp(-w/2+y0)/((2*pi)**(d/2))
            # rho_x0 = cvt(torch.from_numpy(rho_x0))
            x2 = torch.randn((10000, 2)).to(device)

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
start time: 2022_03_22_14_07_34
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.538   1.438e+03  5.666e+00  1.317e+01  9.285e+01     1.190e+03  4.877e+00  1.096e+01  6.990e+01 
00002   0.149   1.190e+03  4.861e+00  1.096e+01  6.980e+01     8.651e+02  3.499e+00  8.131e+00  3.450e+01 
00003   0.152   8.651e+02  3.488e+00  8.132e+00  3.445e+01     6.207e+02  2.279e+00  5.959e+00  1.342e+01 
00004   0.151   6.206e+02  2.271e+00  5.958e+00  1.340e+01     5.032e+02  1.569e+00  4.895e+00  5.913e+00 
00005   0.154   5.029e+02  1.564e+00  4.892e+00  5.895e+00     4.406e+02  1.116e+00  4.322e+00  2.759e+00 
00006   0.151   4.401e+02  1.112e+00  4.318e+00  2.748e+00     4.007e+02  8.101e-01  3.953e+00  1.263e+00 
00007   0.149   4.001e+02  8.070e-01  3.948e+00  1.258e+00     3.727e+02  6.097e-01  3.691e+00  5.611e-01 
00008   0.150   3.721e+02  6.077e-01  3.685e+00  5.589e-01     3.551e+02  4.840e-01  3.524e+00  2.607e-01 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime

import matplotlib.pyplot as plt
import torch
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import *

# from src.OTFlowProblem import *
# from src.birth_death_process import *
from source_without_BD import *

import config

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 100
    def_batch    = 4096
    def_niter    = 10000
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['1d','swissroll','2gaussians','8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='moons'
)

parser.add_argument("--nt"    , type=int, default=8, help="number of time steps") # origin:8
parser.add_argument("--nt_val", type=int, default=8, help="number of time steps for validation") # origin:8
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0') # origin 1, 100, 5
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)

parser.add_argument('--lr'          , type=float, default=0.05)
parser.add_argument("--drop_freq"   , type=int  , default=200, help="how often to decrease learning rate") # origin 100
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='double', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=0)
parser.add_argument('--sample_freq', type=int, default=25)
parser.add_argument('--logrho0_freq', type=int, default=1000)
parser.add_argument('--alphaa', type=float, default=10)

args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]


# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)
print("---------------------------------------------------11111")

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

def compute_loss(net, x,x2, rho_x, nt):


    costC1,costL1, costV1, cs, weights, position = OTFlowProblem_ex(x, net, [0, 1], nt=nt, stepper="rk4",
                                                                    alph=net.alph, is_1d=False, alphaa=args.alphaa,jjj=0,device=device)


    # costC2, costL2, costV2, cs2, weights2, position2 = OTFlowProblem_ex(x2, net, [1, 0], nt=nt, stepper="rk4",
    #                                                                     alph=net.alph, is_1d=False, alphaa=args.alphaa,
    #                                                                     jjj=1, device=device)

    #
    # position4 = position2.cpu().detach().numpy()
    # plt.plot(position4[:, 0], position4[:, 1])
    # plt.show()
    # d=2
    # genModel, _ = integrate_ex(x2[:, 0:d], net, [1, 0.0], nt_val, stepper="rk4", alph=net.alph)
    # costC2 = genModel[0, -4] / x.shape[0]

    # a = torch.sum(genModel[:,-1])
    # b = genModel[:,-1]
    # costC2 = torch.mean(torch.log(genModel[:,-1]))
    # Jc, cs = OTFlowProblem(x, net, [0, 1], nt=nt, stepper="rk4", alph=net.alph, is_1d=True)

    # costL1 = torch.sum(genModel[:,-3])
    # costV1 = torch.sum(genModel[:,-5])
    # print(costC2+costC1, costL1, costV1)
    # print('weights_min:{},weights_max:{}, weights2:{}, weights2:{}'.format(torch.min(weights),torch.max(weights),torch.min(weights2),torch.max(weights2)))

    Jc = (costC1)*100+costL1*5+costV1*1
    return Jc, cs, weights, position


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None

    # setup data [nSamples, d]
    # use one batch as the entire data set
    print(args.data)
    x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=True,device=device)
    x0 = cvt(torch.from_numpy(x0))
    rho_x0 = cvt(torch.from_numpy(rho_x0))
    x2 = torch.randn((4096, 2)).to(device)
    x3 = torch.randn((4096, 2)).to(device)

    x0val, rho_x0val, d_net2 = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size,require_density=True,device=device)
    # x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, require_density=True)
    x0val = cvt(torch.from_numpy(x0val))
    rho_x0val = cvt(torch.from_numpy(rho_x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s} {:9s}     {:9s}  {:9s}  {:9s}  {:9s} {:9s} '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'V(velocity)', 'valLoss', 'valL', 'valC', 'valR',
            'valV'
        )
    )
    logger.info(log_msg)

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs, weights, position = compute_loss(net, x0, x2, rho_x0, nt=args.nt)

        # torch.autograd.set_detect_anomaly(True)
        loss.backward()
        optim.step()

        # position3 = position.cpu().detach().numpy()
        # plt.plot(position3[:, 0], position3[:, 1])
        # plt.show()

        # if itr==1 or itr%100==0:
        #     a = torch.linspace(-4, 4, 1000)
        #     uu = []
        #     for i in range(1000):
        #         uu.append(torch.cat((a[i] * torch.ones_like(a).unsqueeze(-1), a.unsqueeze(-1)), dim=-1))
        #     u = torch.cat(uu).to(device)
        #     t = 0
        #     for i in range(9):
        #         sPath2 = os.path.join(args.save, 'figs2', start_time + '_{}.png'.format(t))
        #         if not os.path.exists(os.path.dirname(sPath2)):
        #             os.makedirs(os.path.dirname(sPath2))
        #         z = pad(u, (0, 1, 0, 0), value=t)
        #         q = net(z).reshape(1000, 1000)
        #         figure = plt.figure()
        #         axes = Axes3D(figure)
        #         XX = np.linspace(-4, 4, 1000)
        #         TT = np.linspace(-4,4,1000)
        #         XX, TT = np.meshgrid(XX, TT)
        #         plt.xlabel("X")
        #         plt.ylabel("T")
        #         plt.title("itr:{}, t:{}".format(0, t))
        #         q = q.cpu().detach().numpy()
        #         axes.plot_surface(XX, TT, q, cmap='rainbow')
        #         plt.savefig(sPath2, dpi=300)
        #         t = t + 0.125

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate
        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs, test_weights, test_position = compute_loss(net, x0val, x3, rho_x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()

        logger.info(log_message)

        # if itr % args.logrho0_freq == 0:
        #     x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, require_density=True, device=device)
        #     x0 = cvt(torch.from_numpy(x0))
        #     rho_x0 = cvt(torch.from_numpy(rho_x0))

        # create plots
        if itr % args.viz_freq == 0:
            nSamples = 20000
            p_samples = toy_data.inf_train_gen(args.data, batch_size=nSamples, require_density=False,device=device)
            p_samples = cvt(torch.from_numpy(p_samples))
            y = cvt(torch.randn(nSamples, d))
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                # plot1d_BD(net, p_samples, y, nt_val, sPath, doPaths=False, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                #             ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh),alphaa=args.alphaa)
                plot2d(net, p_samples, y, nt_val, sPath, doPaths=True,
                          sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                                 ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1],
                                                                        alph[2], nt, m, nTh))
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=False,device=device)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu
            y0 = d_net(x0)
            y0 = y0.detach().cpu().numpy()
            x1 = x0.detach().cpu().numpy()
            w = np.sum(np.power(x1, 2), 1, keepdims=True)
            # rho_x0 = np.exp(-w/2+y0)/((2*pi)**(d/2))
            # rho_x0 = cvt(torch.from_numpy(rho_x0))
            x2 = torch.randn((10000, 2)).to(device)

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
start time: 2022_03_22_14_08_32
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime

import matplotlib.pyplot as plt
import torch
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import *

# from src.OTFlowProblem import *
# from src.birth_death_process import *
from source_without_BD import *

import config

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 100
    def_batch    = 4096
    def_niter    = 10000
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['1d','swissroll','2gaussians','8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='moons'
)

parser.add_argument("--nt"    , type=int, default=8, help="number of time steps") # origin:8
parser.add_argument("--nt_val", type=int, default=8, help="number of time steps for validation") # origin:8
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0') # origin 1, 100, 5
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)

parser.add_argument('--lr'          , type=float, default=0.05)
parser.add_argument("--drop_freq"   , type=int  , default=200, help="how often to decrease learning rate") # origin 100
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='double', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=0)
parser.add_argument('--sample_freq', type=int, default=25)
parser.add_argument('--logrho0_freq', type=int, default=1000)
parser.add_argument('--alphaa', type=float, default=10)

args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]


# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

def compute_loss(net, x,x2, rho_x, nt):


    costC1,costL1, costV1, cs, weights, position = OTFlowProblem_ex(x, net, [0, 1], nt=nt, stepper="rk4",
                                                                    alph=net.alph, is_1d=False, alphaa=args.alphaa,jjj=0,device=device)


    # costC2, costL2, costV2, cs2, weights2, position2 = OTFlowProblem_ex(x2, net, [1, 0], nt=nt, stepper="rk4",
    #                                                                     alph=net.alph, is_1d=False, alphaa=args.alphaa,
    #                                                                     jjj=1, device=device)

    #
    # position4 = position2.cpu().detach().numpy()
    # plt.plot(position4[:, 0], position4[:, 1])
    # plt.show()
    # d=2
    # genModel, _ = integrate_ex(x2[:, 0:d], net, [1, 0.0], nt_val, stepper="rk4", alph=net.alph)
    # costC2 = genModel[0, -4] / x.shape[0]

    # a = torch.sum(genModel[:,-1])
    # b = genModel[:,-1]
    # costC2 = torch.mean(torch.log(genModel[:,-1]))
    # Jc, cs = OTFlowProblem(x, net, [0, 1], nt=nt, stepper="rk4", alph=net.alph, is_1d=True)

    # costL1 = torch.sum(genModel[:,-3])
    # costV1 = torch.sum(genModel[:,-5])
    # print(costC2+costC1, costL1, costV1)
    # print('weights_min:{},weights_max:{}, weights2:{}, weights2:{}'.format(torch.min(weights),torch.max(weights),torch.min(weights2),torch.max(weights2)))

    Jc = (costC1)*100+costL1*5+costV1*1
    return Jc, cs, weights, position


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None

    # setup data [nSamples, d]
    # use one batch as the entire data set
    print(args.data)
    x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=True,device=device)
    x0 = cvt(torch.from_numpy(x0))
    rho_x0 = cvt(torch.from_numpy(rho_x0))
    x2 = torch.randn((4096, 2)).to(device)
    x3 = torch.randn((4096, 2)).to(device)

    x0val, rho_x0val, d_net2 = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size,require_density=True,device=device)
    # x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, require_density=True)
    x0val = cvt(torch.from_numpy(x0val))
    rho_x0val = cvt(torch.from_numpy(rho_x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s} {:9s}     {:9s}  {:9s}  {:9s}  {:9s} {:9s} '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'V(velocity)', 'valLoss', 'valL', 'valC', 'valR',
            'valV'
        )
    )
    logger.info(log_msg)

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs, weights, position = compute_loss(net, x0, x2, rho_x0, nt=args.nt)

        # torch.autograd.set_detect_anomaly(True)
        loss.backward()
        optim.step()

        # position3 = position.cpu().detach().numpy()
        # plt.plot(position3[:, 0], position3[:, 1])
        # plt.show()

        # if itr==1 or itr%100==0:
        #     a = torch.linspace(-4, 4, 1000)
        #     uu = []
        #     for i in range(1000):
        #         uu.append(torch.cat((a[i] * torch.ones_like(a).unsqueeze(-1), a.unsqueeze(-1)), dim=-1))
        #     u = torch.cat(uu).to(device)
        #     t = 0
        #     for i in range(9):
        #         sPath2 = os.path.join(args.save, 'figs2', start_time + '_{}.png'.format(t))
        #         if not os.path.exists(os.path.dirname(sPath2)):
        #             os.makedirs(os.path.dirname(sPath2))
        #         z = pad(u, (0, 1, 0, 0), value=t)
        #         q = net(z).reshape(1000, 1000)
        #         figure = plt.figure()
        #         axes = Axes3D(figure)
        #         XX = np.linspace(-4, 4, 1000)
        #         TT = np.linspace(-4,4,1000)
        #         XX, TT = np.meshgrid(XX, TT)
        #         plt.xlabel("X")
        #         plt.ylabel("T")
        #         plt.title("itr:{}, t:{}".format(0, t))
        #         q = q.cpu().detach().numpy()
        #         axes.plot_surface(XX, TT, q, cmap='rainbow')
        #         plt.savefig(sPath2, dpi=300)
        #         t = t + 0.125

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate
        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs, test_weights, test_position = compute_loss(net, x0val, x3, rho_x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()

        logger.info(log_message)

        # if itr % args.logrho0_freq == 0:
        #     x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, require_density=True, device=device)
        #     x0 = cvt(torch.from_numpy(x0))
        #     rho_x0 = cvt(torch.from_numpy(rho_x0))

        # create plots
        if itr % args.viz_freq == 0:
            nSamples = 20000
            p_samples = toy_data.inf_train_gen(args.data, batch_size=nSamples, require_density=False,device=device)
            p_samples = cvt(torch.from_numpy(p_samples))
            y = cvt(torch.randn(nSamples, d))
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                # plot1d_BD(net, p_samples, y, nt_val, sPath, doPaths=False, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                #             ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh),alphaa=args.alphaa)
                plot2d(net, p_samples, y, nt_val, sPath, doPaths=True,
                          sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                                 ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1],
                                                                        alph[2], nt, m, nTh))
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=False,device=device)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu
            y0 = d_net(x0)
            y0 = y0.detach().cpu().numpy()
            x1 = x0.detach().cpu().numpy()
            w = np.sum(np.power(x1, 2), 1, keepdims=True)
            # rho_x0 = np.exp(-w/2+y0)/((2*pi)**(d/2))
            # rho_x0 = cvt(torch.from_numpy(rho_x0))
            x2 = torch.randn((10000, 2)).to(device)

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
start time: 2022_03_22_14_09_21
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.587   1.265e+03  4.507e+00  1.186e+01  5.701e+01     1.098e+03  3.793e+00  1.040e+01  3.915e+01 
00002   0.172   1.098e+03  3.791e+00  1.040e+01  3.928e+01     7.761e+02  2.442e+00  7.489e+00  1.498e+01 
00003   0.165   7.761e+02  2.441e+00  7.489e+00  1.504e+01     5.911e+02  1.620e+00  5.771e+00  5.869e+00 
00004   0.162   5.910e+02  1.619e+00  5.770e+00  5.891e+00     5.051e+02  1.191e+00  4.962e+00  2.929e+00 
00005   0.168   5.050e+02  1.191e+00  4.961e+00  2.940e+00     4.559e+02  9.103e-01  4.498e+00  1.510e+00 
00006   0.164   4.557e+02  9.104e-01  4.496e+00  1.515e+00     4.231e+02  6.885e-01  4.189e+00  7.480e-01 
00007   0.163   4.228e+02  6.887e-01  4.186e+00  7.507e-01     4.023e+02  5.361e-01  3.992e+00  3.857e-01 
00008   0.158   4.020e+02  5.362e-01  3.989e+00  3.873e-01     3.884e+02  4.431e-01  3.860e+00  2.367e-01 
00009   0.153   3.881e+02  4.430e-01  3.857e+00  2.376e-01     3.772e+02  3.939e-01  3.750e+00  1.893e-01 
00010   0.157   3.769e+02  3.937e-01  3.747e+00  1.897e-01     3.693e+02  4.115e-01  3.671e+00  2.007e-01 
00011   0.170   3.691e+02  4.111e-01  3.668e+00  2.006e-01     3.620e+02  4.501e-01  3.595e+00  2.458e-01 
00012   0.163   3.618e+02  4.490e-01  3.593e+00  2.452e-01     3.573e+02  5.035e-01  3.545e+00  3.209e-01 
00013   0.162   3.572e+02  5.018e-01  3.544e+00  3.196e-01     3.529e+02  5.255e-01  3.499e+00  4.014e-01 
00014   0.165   3.529e+02  5.233e-01  3.499e+00  3.995e-01     3.493e+02  5.289e-01  3.462e+00  4.529e-01 
00015   0.155   3.493e+02  5.266e-01  3.462e+00  4.506e-01     3.455e+02  5.190e-01  3.424e+00  4.474e-01 
00016   0.177   3.455e+02  5.167e-01  3.425e+00  4.450e-01     3.418e+02  4.902e-01  3.390e+00  3.956e-01 
00017   0.163   3.419e+02  4.880e-01  3.390e+00  3.934e-01     3.395e+02  4.394e-01  3.369e+00  3.347e-01 
00018   0.163   3.395e+02  4.373e-01  3.370e+00  3.328e-01     3.377e+02  3.801e-01  3.355e+00  2.881e-01 
00019   0.167   3.377e+02  3.782e-01  3.355e+00  2.865e-01     3.369e+02  3.529e-01  3.349e+00  2.597e-01 
00020   0.169   3.369e+02  3.512e-01  3.349e+00  2.582e-01     3.362e+02  3.520e-01  3.342e+00  2.415e-01 
00021   0.151   3.362e+02  3.505e-01  3.342e+00  2.401e-01     3.356e+02  3.559e-01  3.336e+00  2.317e-01 
00022   0.154   3.357e+02  3.543e-01  3.337e+00  2.304e-01     3.350e+02  3.527e-01  3.330e+00  2.354e-01 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime

import matplotlib.pyplot as plt
import torch
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import *

# from src.OTFlowProblem import *
# from src.birth_death_process import *
from source_without_BD import *

import config

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 100
    def_batch    = 4096
    def_niter    = 10000
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['1d','swissroll','2gaussians','8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='moons'
)

parser.add_argument("--nt"    , type=int, default=8, help="number of time steps") # origin:8
parser.add_argument("--nt_val", type=int, default=8, help="number of time steps for validation") # origin:8
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0') # origin 1, 100, 5
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)

parser.add_argument('--lr'          , type=float, default=0.05)
parser.add_argument("--drop_freq"   , type=int  , default=200, help="how often to decrease learning rate") # origin 100
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='double', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=0)
parser.add_argument('--sample_freq', type=int, default=25)
parser.add_argument('--logrho0_freq', type=int, default=1000)
parser.add_argument('--alphaa', type=float, default=10)

args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]


# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

def compute_loss(net, x,x2, rho_x, nt):


    costC1,costL1, costV1, cs, weights, position = OTFlowProblem_ex(x, net, [0, 1], nt=nt, stepper="rk4",
                                                                    alph=net.alph, is_1d=False, alphaa=args.alphaa,jjj=0,device=device)


    # costC2, costL2, costV2, cs2, weights2, position2 = OTFlowProblem_ex(x2, net, [1, 0], nt=nt, stepper="rk4",
    #                                                                     alph=net.alph, is_1d=False, alphaa=args.alphaa,
    #                                                                     jjj=1, device=device)

    #
    # position4 = position2.cpu().detach().numpy()
    # plt.plot(position4[:, 0], position4[:, 1])
    # plt.show()
    # d=2
    # genModel, _ = integrate_ex(x2[:, 0:d], net, [1, 0.0], nt_val, stepper="rk4", alph=net.alph)
    # costC2 = genModel[0, -4] / x.shape[0]

    # a = torch.sum(genModel[:,-1])
    # b = genModel[:,-1]
    # costC2 = torch.mean(torch.log(genModel[:,-1]))
    # Jc, cs = OTFlowProblem(x, net, [0, 1], nt=nt, stepper="rk4", alph=net.alph, is_1d=True)

    # costL1 = torch.sum(genModel[:,-3])
    # costV1 = torch.sum(genModel[:,-5])
    # print(costC2+costC1, costL1, costV1)
    # print('weights_min:{},weights_max:{}, weights2:{}, weights2:{}'.format(torch.min(weights),torch.max(weights),torch.min(weights2),torch.max(weights2)))

    Jc = (costC1)*100+costL1*5+costV1*1
    return Jc, cs, weights, position


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None

    # setup data [nSamples, d]
    # use one batch as the entire data set
    print(args.data)
    x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=True,device=device)
    x0 = cvt(torch.from_numpy(x0))
    rho_x0 = cvt(torch.from_numpy(rho_x0))
    x2 = torch.randn((4096, 2)).to(device)
    x3 = torch.randn((4096, 2)).to(device)

    x0val, rho_x0val, d_net2 = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size,require_density=True,device=device)
    # x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, require_density=True)
    x0val = cvt(torch.from_numpy(x0val))
    rho_x0val = cvt(torch.from_numpy(rho_x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s} {:9s}     {:9s}  {:9s}  {:9s}  {:9s} {:9s} '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'V(velocity)', 'valLoss', 'valL', 'valC', 'valR',
            'valV'
        )
    )
    logger.info(log_msg)

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs, weights, position = compute_loss(net, x0, x2, rho_x0, nt=args.nt)

        # torch.autograd.set_detect_anomaly(True)
        loss.backward()
        optim.step()

        # position3 = position.cpu().detach().numpy()
        # plt.plot(position3[:, 0], position3[:, 1])
        # plt.show()

        # if itr==1 or itr%100==0:
        #     a = torch.linspace(-4, 4, 1000)
        #     uu = []
        #     for i in range(1000):
        #         uu.append(torch.cat((a[i] * torch.ones_like(a).unsqueeze(-1), a.unsqueeze(-1)), dim=-1))
        #     u = torch.cat(uu).to(device)
        #     t = 0
        #     for i in range(9):
        #         sPath2 = os.path.join(args.save, 'figs2', start_time + '_{}.png'.format(t))
        #         if not os.path.exists(os.path.dirname(sPath2)):
        #             os.makedirs(os.path.dirname(sPath2))
        #         z = pad(u, (0, 1, 0, 0), value=t)
        #         q = net(z).reshape(1000, 1000)
        #         figure = plt.figure()
        #         axes = Axes3D(figure)
        #         XX = np.linspace(-4, 4, 1000)
        #         TT = np.linspace(-4,4,1000)
        #         XX, TT = np.meshgrid(XX, TT)
        #         plt.xlabel("X")
        #         plt.ylabel("T")
        #         plt.title("itr:{}, t:{}".format(0, t))
        #         q = q.cpu().detach().numpy()
        #         axes.plot_surface(XX, TT, q, cmap='rainbow')
        #         plt.savefig(sPath2, dpi=300)
        #         t = t + 0.125

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate
        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs, test_weights, test_position = compute_loss(net, x0val, x3, rho_x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()

        logger.info(log_message)

        # if itr % args.logrho0_freq == 0:
        #     x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, require_density=True, device=device)
        #     x0 = cvt(torch.from_numpy(x0))
        #     rho_x0 = cvt(torch.from_numpy(rho_x0))

        # create plots
        if itr % args.viz_freq == 0:
            nSamples = 20000
            p_samples = toy_data.inf_train_gen(args.data, batch_size=nSamples, require_density=False,device=device)
            p_samples = cvt(torch.from_numpy(p_samples))
            y = cvt(torch.randn(nSamples, d))
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                # plot1d_BD(net, p_samples, y, nt_val, sPath, doPaths=False, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                #             ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh),alphaa=args.alphaa)
                plot2d(net, p_samples, y, nt_val, sPath, doPaths=True,
                          sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                                 ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1],
                                                                        alph[2], nt, m, nTh))
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=False,device=device)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu
            y0 = d_net(x0)
            y0 = y0.detach().cpu().numpy()
            x1 = x0.detach().cpu().numpy()
            w = np.sum(np.power(x1, 2), 1, keepdims=True)
            # rho_x0 = np.exp(-w/2+y0)/((2*pi)**(d/2))
            # rho_x0 = cvt(torch.from_numpy(rho_x0))
            x2 = torch.randn((10000, 2)).to(device)

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
start time: 2022_03_22_14_09_56
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
# trainToyOTflow.py
# training driver for the two-dimensional toy problems
import argparse
import os
import time
import datetime

import matplotlib.pyplot as plt
import torch
import torch.optim as optim
import numpy as np
import math
import lib.toy_data as toy_data
import lib.utils as utils
from lib.utils import count_parameters
from src.plotter import *

# from src.OTFlowProblem import *
# from src.birth_death_process import *
from source_without_BD import *

import config

cf = config.getconfig()

if cf.gpu: # if gpu on platform
    def_viz_freq = 100
    def_batch    = 4096
    def_niter    = 10000
else:  # if no gpu on platform, assume debugging on a local cpu
    def_viz_freq = 100
    def_batch    = 2048
    def_niter    = 1000

parser = argparse.ArgumentParser('OT-Flow')
parser.add_argument(
    '--data', choices=['1d','swissroll','2gaussians','8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='moons'
)

parser.add_argument("--nt"    , type=int, default=8, help="number of time steps") # origin:8
parser.add_argument("--nt_val", type=int, default=8, help="number of time steps for validation") # origin:8
parser.add_argument('--alph'  , type=str, default='1.0,100.0,5.0') # origin 1, 100, 5
parser.add_argument('--m'     , type=int, default=32)
parser.add_argument('--nTh'   , type=int, default=2)

parser.add_argument('--niters'        , type=int  , default=def_niter)
parser.add_argument('--batch_size'    , type=int  , default=def_batch)
parser.add_argument('--val_batch_size', type=int  , default=def_batch)

parser.add_argument('--lr'          , type=float, default=0.05)
parser.add_argument("--drop_freq"   , type=int  , default=200, help="how often to decrease learning rate") # origin 100
parser.add_argument('--weight_decay', type=float, default=0.0)
parser.add_argument('--lr_drop'     , type=float, default=2.0)
parser.add_argument('--optim'       , type=str  , default='adam', choices=['adam'])
parser.add_argument('--prec'        , type=str  , default='double', choices=['single','double'], help="single or double precision")

parser.add_argument('--save'    , type=str, default='experiments/cnf/toy')
parser.add_argument('--viz_freq', type=int, default=def_viz_freq)
parser.add_argument('--val_freq', type=int, default=1)
parser.add_argument('--gpu'     , type=int, default=0)
parser.add_argument('--sample_freq', type=int, default=25)
parser.add_argument('--logrho0_freq', type=int, default=1000)
parser.add_argument('--alphaa', type=float, default=10)

args = parser.parse_args()

args.alph = [float(item) for item in args.alph.split(',')]


# get precision type
if args.prec =='double':
    prec = torch.float64
else:
    prec = torch.float32

# get timestamp for saving models
start_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
logger.info("start time: " + start_time)
logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

def compute_loss(net, x,x2, rho_x, nt):


    costC1,costL1, costV1, cs, weights, position = OTFlowProblem_ex(x, net, [0, 1], nt=nt, stepper="rk4",
                                                                    alph=net.alph, is_1d=False, alphaa=args.alphaa,jjj=0,device=device)


    # costC2, costL2, costV2, cs2, weights2, position2 = OTFlowProblem_ex(x2, net, [1, 0], nt=nt, stepper="rk4",
    #                                                                     alph=net.alph, is_1d=False, alphaa=args.alphaa,
    #                                                                     jjj=1, device=device)

    #
    # position4 = position2.cpu().detach().numpy()
    # plt.plot(position4[:, 0], position4[:, 1])
    # plt.show()
    # d=2
    # genModel, _ = integrate_ex(x2[:, 0:d], net, [1, 0.0], nt_val, stepper="rk4", alph=net.alph)
    # costC2 = genModel[0, -4] / x.shape[0]

    # a = torch.sum(genModel[:,-1])
    # b = genModel[:,-1]
    # costC2 = torch.mean(torch.log(genModel[:,-1]))
    # Jc, cs = OTFlowProblem(x, net, [0, 1], nt=nt, stepper="rk4", alph=net.alph, is_1d=True)

    # costL1 = torch.sum(genModel[:,-3])
    # costV1 = torch.sum(genModel[:,-5])
    # print(costC2+costC1, costL1, costV1)
    # print('weights_min:{},weights_max:{}, weights2:{}, weights2:{}'.format(torch.min(weights),torch.max(weights),torch.min(weights2),torch.max(weights2)))

    Jc = (costC1)*100+costL1*5+costV1*1
    return Jc, cs, weights, position


if __name__ == '__main__':

    torch.set_default_dtype(prec)
    cvt = lambda x: x.type(prec).to(device, non_blocking=True)

    # neural network for the potential function Phi
    d      = 2
    alph   = args.alph
    nt     = args.nt
    nt_val = args.nt_val
    nTh    = args.nTh
    m      = args.m
    net = Phi(nTh=nTh, m=args.m, d=d, alph=alph)
    net = net.to(prec).to(device)

    optim = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay ) # lr=0.04 good

    logger.info(net)
    logger.info("-------------------------")
    logger.info("DIMENSION={:}  m={:}  nTh={:}   alpha={:}".format(d,m,nTh,alph))
    logger.info("nt={:}   nt_val={:}".format(nt,nt_val))
    logger.info("Number of trainable parameters: {}".format(count_parameters(net)))
    logger.info("-------------------------")
    logger.info(str(optim)) # optimizer info
    logger.info("data={:} batch_size={:} gpu={:}".format(args.data, args.batch_size, args.gpu))
    logger.info("maxIters={:} val_freq={:} viz_freq={:}".format(args.niters, args.val_freq, args.viz_freq))
    logger.info("saveLocation = {:}".format(args.save))
    logger.info("-------------------------\n")

    end = time.time()
    best_loss = float('inf')
    bestParams = None

    # setup data [nSamples, d]
    # use one batch as the entire data set
    print(args.data)
    x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=True,device=device)
    x0 = cvt(torch.from_numpy(x0))
    rho_x0 = cvt(torch.from_numpy(rho_x0))
    x2 = torch.randn((4096, 2)).to(device)
    x3 = torch.randn((4096, 2)).to(device)

    x0val, rho_x0val, d_net2 = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size,require_density=True,device=device)
    # x0val = toy_data.inf_train_gen(args.data, batch_size=args.val_batch_size, require_density=True)
    x0val = cvt(torch.from_numpy(x0val))
    rho_x0val = cvt(torch.from_numpy(rho_x0val))

    log_msg = (
        '{:5s}  {:6s}   {:9s}  {:9s}  {:9s}  {:9s} {:9s}     {:9s}  {:9s}  {:9s}  {:9s} {:9s} '.format(
            'iter', ' time','loss', 'L (L_2)', 'C (loss)', 'R (HJB)', 'V(velocity)', 'valLoss', 'valL', 'valC', 'valR',
            'valV'
        )
    )
    logger.info(log_msg)

    time_meter = utils.AverageMeter()

    net.train()
    for itr in range(1, args.niters + 1):
        # train
        optim.zero_grad()
        loss, costs, weights, position = compute_loss(net, x0, x2, rho_x0, nt=args.nt)

        # torch.autograd.set_detect_anomaly(True)
        loss.backward()
        optim.step()

        # position3 = position.cpu().detach().numpy()
        # plt.plot(position3[:, 0], position3[:, 1])
        # plt.show()

        # if itr==1 or itr%100==0:
        #     a = torch.linspace(-4, 4, 1000)
        #     uu = []
        #     for i in range(1000):
        #         uu.append(torch.cat((a[i] * torch.ones_like(a).unsqueeze(-1), a.unsqueeze(-1)), dim=-1))
        #     u = torch.cat(uu).to(device)
        #     t = 0
        #     for i in range(9):
        #         sPath2 = os.path.join(args.save, 'figs2', start_time + '_{}.png'.format(t))
        #         if not os.path.exists(os.path.dirname(sPath2)):
        #             os.makedirs(os.path.dirname(sPath2))
        #         z = pad(u, (0, 1, 0, 0), value=t)
        #         q = net(z).reshape(1000, 1000)
        #         figure = plt.figure()
        #         axes = Axes3D(figure)
        #         XX = np.linspace(-4, 4, 1000)
        #         TT = np.linspace(-4,4,1000)
        #         XX, TT = np.meshgrid(XX, TT)
        #         plt.xlabel("X")
        #         plt.ylabel("T")
        #         plt.title("itr:{}, t:{}".format(0, t))
        #         q = q.cpu().detach().numpy()
        #         axes.plot_surface(XX, TT, q, cmap='rainbow')
        #         plt.savefig(sPath2, dpi=300)
        #         t = t + 0.125

        time_meter.update(time.time() - end)

        log_message = (
            '{:05d}  {:6.3f}   {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                itr, time_meter.val , loss, costs[0], costs[1], costs[2]
            )
        )

        # validate
        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                net.eval()
                test_loss, test_costs, test_weights, test_position = compute_loss(net, x0val, x3, rho_x0val, nt=nt_val)

                # add to print message
                log_message += '    {:9.3e}  {:9.3e}  {:9.3e}  {:9.3e} '.format(
                    test_loss, test_costs[0], test_costs[1], test_costs[2]
                )

                # save best set of parameters
                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    best_costs = test_costs
                    utils.makedirs(args.save)
                    best_params = net.state_dict()
                    torch.save({
                        'args': args,
                        'state_dict': best_params,
                    }, os.path.join(args.save, start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m)))
                    net.train()

        logger.info(log_message)

        # if itr % args.logrho0_freq == 0:
        #     x0, rho_x0, d_net = toy_data.inf_train_gen(args.data, batch_size=args.batch_size, require_density=True, device=device)
        #     x0 = cvt(torch.from_numpy(x0))
        #     rho_x0 = cvt(torch.from_numpy(rho_x0))

        # create plots
        if itr % args.viz_freq == 0:
            nSamples = 20000
            p_samples = toy_data.inf_train_gen(args.data, batch_size=nSamples, require_density=False,device=device)
            p_samples = cvt(torch.from_numpy(p_samples))
            y = cvt(torch.randn(nSamples, d))
            with torch.no_grad():
                net.eval()
                curr_state = net.state_dict()
                net.load_state_dict(best_params)

                sPath = os.path.join(args.save, 'figs', start_time + '_{:04d}.png'.format(itr))
                # plot1d_BD(net, p_samples, y, nt_val, sPath, doPaths=False, sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                #             ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1], alph[2], nt, m, nTh),alphaa=args.alphaa)
                plot2d(net, p_samples, y, nt_val, sPath, doPaths=True,
                          sTitle='{:s}  -  loss {:.2f}  ,  C {:.2f}  ,  alph {:.1f} {:.1f}  '
                                 ' nt {:d}   m {:d}  nTh {:d}  '.format(args.data, best_loss, best_costs[1], alph[1],
                                                                        alph[2], nt, m, nTh))
                net.load_state_dict(curr_state)
                net.train()

        # shrink step size
        if itr % args.drop_freq == 0:
            for p in optim.param_groups:
                p['lr'] /= args.lr_drop
            print("lr: ", p['lr'])

        # resample data
        if itr % args.sample_freq == 0:
            # resample data [nSamples, d+1]
            logger.info("resampling")
            x0 = toy_data.inf_train_gen(args.data, batch_size=args.batch_size,require_density=False,device=device)  # load data batch
            x0 = cvt(torch.from_numpy(x0))  # convert to torch, type and gpu
            y0 = d_net(x0)
            y0 = y0.detach().cpu().numpy()
            x1 = x0.detach().cpu().numpy()
            w = np.sum(np.power(x1, 2), 1, keepdims=True)
            # rho_x0 = np.exp(-w/2+y0)/((2*pi)**(d/2))
            # rho_x0 = cvt(torch.from_numpy(rho_x0))
            x2 = torch.randn((10000, 2)).to(device)

        end = time.time()

    logger.info("Training Time: {:} seconds".format(time_meter.sum))
    logger.info('Training has finished.  ' + start_time + '_{:}_alph{:}_{:}_m{:}_checkpt.pth'.format(args.data,int(alph[1]),int(alph[2]),m))
start time: 2022_03_22_14_10_45
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_22_14_11_21
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.574   1.261e+03  4.251e+00  1.194e+01  4.596e+01     1.126e+03  4.009e+00  1.066e+01  3.975e+01 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_22_14_11_40
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.579   1.047e+03  3.945e+00  9.871e+00  3.988e+01     1.062e+03  4.184e+00  9.944e+00  4.623e+01 
00002   0.170   1.061e+03  4.161e+00  9.944e+00  4.597e+01     6.544e+02  2.391e+00  6.309e+00  1.152e+01 
00003   0.174   6.543e+02  2.378e+00  6.309e+00  1.146e+01     4.813e+02  1.533e+00  4.699e+00  3.709e+00 
00004   0.155   4.812e+02  1.526e+00  4.698e+00  3.690e+00     4.200e+02  1.169e+00  4.120e+00  2.068e+00 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_23_01_20_51
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.607   1.167e+03  4.458e+00  1.100e+01  4.414e+01     1.076e+03  4.135e+00  1.017e+01  3.820e+01 
00002   0.161   1.076e+03  4.119e+00  1.017e+01  3.809e+01     7.093e+02  2.540e+00  6.854e+00  1.117e+01 
00003   0.155   7.092e+02  2.530e+00  6.854e+00  1.114e+01     4.820e+02  1.355e+00  4.730e+00  2.237e+00 
00004   0.154   4.820e+02  1.350e+00  4.730e+00  2.233e+00     4.022e+02  7.991e-01  3.975e+00  6.802e-01 
00005   0.153   4.020e+02  7.965e-01  3.974e+00  6.801e-01     3.706e+02  5.141e-01  3.678e+00  2.919e-01 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_23_01_27_22
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.531   1.203e+03  4.937e+00  1.117e+01  6.115e+01     1.154e+03  4.811e+00  1.068e+01  6.149e+01 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_23_01_31_16
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_23_01_31_32
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.503   1.317e+03  5.418e+00  1.221e+01  6.897e+01     1.071e+03  4.310e+00  1.008e+01  4.174e+01 
00002   0.153   1.071e+03  4.339e+00  1.008e+01  4.199e+01     7.684e+02  2.952e+00  7.361e+00  1.753e+01 
00003   0.151   7.685e+02  2.973e+00  7.360e+00  1.764e+01     5.216e+02  1.710e+00  5.083e+00  4.834e+00 
00004   0.158   5.217e+02  1.723e+00  5.083e+00  4.868e+00     4.087e+02  1.028e+00  4.020e+00  1.547e+00 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_23_01_32_08
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_23_10_37_55
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.572   1.146e+03  4.111e+00  1.074e+01  5.096e+01     9.599e+02  3.555e+00  9.067e+00  3.539e+01 
00002   0.162   9.599e+02  3.553e+00  9.067e+00  3.545e+01     6.595e+02  2.298e+00  6.358e+00  1.226e+01 
00003   0.156   6.595e+02  2.298e+00  6.357e+00  1.229e+01     4.889e+02  1.431e+00  4.780e+00  3.767e+00 
00004   0.155   4.888e+02  1.432e+00  4.779e+00  3.780e+00     3.987e+02  8.892e-01  3.931e+00  1.156e+00 
00005   0.159   3.985e+02  8.892e-01  3.929e+00  1.160e+00     3.591e+02  5.585e-01  3.559e+00  4.605e-01 
00006   0.156   3.590e+02  5.581e-01  3.558e+00  4.612e-01     3.465e+02  3.383e-01  3.446e+00  1.910e-01 
00007   0.176   3.463e+02  3.378e-01  3.444e+00  1.911e-01     3.449e+02  3.405e-01  3.431e+00  1.089e-01 
00008   0.170   3.448e+02  3.402e-01  3.430e+00  1.089e-01     3.453e+02  2.050e-01  3.442e+00  6.706e-02 
00009   0.180   3.450e+02  2.052e-01  3.439e+00  6.723e-02     3.347e+02  3.273e-01  3.330e+00  9.700e-02 
00010   0.168   3.346e+02  3.271e-01  3.328e+00  9.722e-02     3.366e+02  5.511e-01  3.337e+00  1.926e-01 
00011   0.161   3.365e+02  5.509e-01  3.335e+00  1.930e-01     3.394e+02  5.367e-01  3.363e+00  3.763e-01 
00012   0.153   3.390e+02  5.370e-01  3.359e+00  3.776e-01     3.414e+02  6.498e-01  3.376e+00  5.521e-01 
00013   0.154   3.410e+02  6.507e-01  3.372e+00  5.543e-01     3.387e+02  6.719e-01  3.348e+00  4.735e-01 
00014   0.156   3.384e+02  6.724e-01  3.346e+00  4.753e-01     3.381e+02  5.144e-01  3.352e+00  3.197e-01 
00015   0.159   3.378e+02  5.144e-01  3.349e+00  3.208e-01     3.321e+02  4.256e-01  3.297e+00  2.532e-01 
00016   0.168   3.318e+02  4.260e-01  3.294e+00  2.543e-01     3.358e+02  3.752e-01  3.337e+00  2.107e-01 
00017   0.178   3.355e+02  3.758e-01  3.334e+00  2.117e-01     3.369e+02  2.537e-01  3.355e+00  1.442e-01 
00018   0.157   3.365e+02  2.539e-01  3.351e+00  1.448e-01     3.361e+02  3.548e-01  3.342e+00  1.563e-01 
00019   0.154   3.359e+02  3.547e-01  3.340e+00  1.568e-01     3.337e+02  3.578e-01  3.317e+00  1.912e-01 
00020   0.156   3.335e+02  3.579e-01  3.315e+00  1.919e-01     3.350e+02  3.319e-01  3.331e+00  2.408e-01 
00021   0.153   3.346e+02  3.322e-01  3.327e+00  2.417e-01     3.323e+02  4.286e-01  3.299e+00  3.032e-01 
00022   0.163   3.320e+02  4.289e-01  3.296e+00  3.043e-01     3.341e+02  5.326e-01  3.311e+00  3.545e-01 
00023   0.155   3.339e+02  5.327e-01  3.309e+00  3.556e-01     3.340e+02  4.733e-01  3.313e+00  3.823e-01 
00024   0.156   3.337e+02  4.734e-01  3.310e+00  3.834e-01     3.333e+02  4.870e-01  3.305e+00  4.119e-01 
00025   0.162   3.330e+02  4.874e-01  3.301e+00  4.132e-01     3.337e+02  5.307e-01  3.307e+00  3.991e-01 
resampling
00026   0.157   3.336e+02  5.290e-01  3.306e+00  3.982e-01     3.325e+02  4.416e-01  3.300e+00  3.245e-01 
00027   0.171   3.324e+02  4.396e-01  3.299e+00  3.236e-01     3.320e+02  4.238e-01  3.296e+00  2.816e-01 
00028   0.172   3.319e+02  4.219e-01  3.295e+00  2.809e-01     3.323e+02  4.329e-01  3.298e+00  2.617e-01 
00029   0.154   3.322e+02  4.314e-01  3.297e+00  2.611e-01     3.321e+02  3.452e-01  3.301e+00  2.179e-01 
00030   0.158   3.319e+02  3.438e-01  3.300e+00  2.174e-01     3.316e+02  3.406e-01  3.297e+00  1.956e-01 
00031   0.155   3.315e+02  3.391e-01  3.296e+00  1.951e-01     3.322e+02  3.965e-01  3.301e+00  1.959e-01 
00032   0.164   3.321e+02  3.949e-01  3.300e+00  1.954e-01     3.312e+02  3.505e-01  3.293e+00  1.897e-01 
00033   0.160   3.311e+02  3.489e-01  3.291e+00  1.893e-01     3.314e+02  3.479e-01  3.295e+00  1.979e-01 
00034   0.174   3.313e+02  3.464e-01  3.294e+00  1.975e-01     3.308e+02  4.110e-01  3.285e+00  2.171e-01 
00035   0.156   3.307e+02  4.094e-01  3.284e+00  2.167e-01     3.304e+02  4.264e-01  3.280e+00  2.269e-01 
00036   0.153   3.303e+02  4.245e-01  3.279e+00  2.264e-01     3.303e+02  4.164e-01  3.280e+00  2.355e-01 
00037   0.153   3.302e+02  4.145e-01  3.279e+00  2.350e-01     3.299e+02  4.540e-01  3.274e+00  2.544e-01 
00038   0.158   3.298e+02  4.522e-01  3.273e+00  2.540e-01     3.296e+02  4.610e-01  3.271e+00  2.547e-01 
00039   0.153   3.295e+02  4.592e-01  3.270e+00  2.543e-01     3.294e+02  4.391e-01  3.270e+00  2.367e-01 
00040   0.156   3.293e+02  4.372e-01  3.269e+00  2.363e-01     3.287e+02  4.598e-01  3.261e+00  2.305e-01 
00041   0.154   3.286e+02  4.580e-01  3.261e+00  2.303e-01     3.280e+02  4.422e-01  3.256e+00  2.178e-01 
00042   0.159   3.280e+02  4.405e-01  3.256e+00  2.177e-01     3.276e+02  4.043e-01  3.254e+00  1.961e-01 
00043   0.162   3.275e+02  4.027e-01  3.253e+00  1.960e-01     3.267e+02  4.255e-01  3.244e+00  1.864e-01 
00044   0.170   3.267e+02  4.237e-01  3.244e+00  1.864e-01     3.260e+02  4.184e-01  3.238e+00  1.742e-01 
00045   0.164   3.261e+02  4.166e-01  3.238e+00  1.743e-01     3.252e+02  3.860e-01  3.231e+00  1.639e-01 
00046   0.162   3.253e+02  3.844e-01  3.232e+00  1.641e-01     3.241e+02  4.088e-01  3.219e+00  1.675e-01 
00047   0.165   3.242e+02  4.073e-01  3.220e+00  1.679e-01     3.227e+02  4.122e-01  3.205e+00  1.612e-01 
00048   0.163   3.228e+02  4.106e-01  3.206e+00  1.617e-01     3.214e+02  4.127e-01  3.191e+00  1.588e-01 
00049   0.159   3.215e+02  4.111e-01  3.193e+00  1.595e-01     3.197e+02  4.463e-01  3.173e+00  1.732e-01 
00050   0.152   3.199e+02  4.448e-01  3.175e+00  1.742e-01     3.178e+02  4.329e-01  3.154e+00  1.662e-01 
resampling
00051   0.154   3.178e+02  4.309e-01  3.155e+00  1.663e-01     3.156e+02  4.692e-01  3.131e+00  1.606e-01 
00052   0.157   3.157e+02  4.674e-01  3.132e+00  1.608e-01     3.131e+02  4.384e-01  3.108e+00  1.487e-01 
00053   0.151   3.131e+02  4.364e-01  3.108e+00  1.489e-01     3.108e+02  4.315e-01  3.085e+00  1.449e-01 
00054   0.151   3.109e+02  4.295e-01  3.086e+00  1.452e-01     3.089e+02  4.478e-01  3.065e+00  1.381e-01 
00055   0.155   3.088e+02  4.460e-01  3.065e+00  1.385e-01     3.076e+02  4.254e-01  3.054e+00  1.438e-01 
00056   0.154   3.076e+02  4.235e-01  3.054e+00  1.442e-01     3.085e+02  4.985e-01  3.058e+00  1.693e-01 
00057   0.156   3.084e+02  4.963e-01  3.058e+00  1.698e-01     3.196e+02  4.760e-01  3.170e+00  2.008e-01 
00058   0.160   3.199e+02  4.736e-01  3.173e+00  2.016e-01     3.923e+02  1.463e+00  3.846e+00  3.958e-01 
00059   0.162   3.912e+02  1.459e+00  3.835e+00  3.955e-01     3.715e+02  9.137e-01  3.662e+00  7.606e-01 
00060   0.153   3.718e+02  9.083e-01  3.665e+00  7.676e-01     4.415e+02  1.164e+00  4.347e+00  9.621e-01 
00061   0.155   4.418e+02  1.159e+00  4.351e+00  9.700e-01     3.754e+02  8.954e-01  3.703e+00  6.036e-01 
00062   0.154   3.754e+02  8.910e-01  3.703e+00  6.092e-01     3.487e+02  8.582e-01  3.443e+00  1.840e-01 
00063   0.154   3.488e+02  8.556e-01  3.443e+00  1.861e-01     3.371e+02  7.141e-01  3.335e+00  4.929e-02 
00064   0.155   3.368e+02  7.129e-01  3.332e+00  4.969e-02     3.472e+02  4.934e-01  3.447e+00  4.061e-02 
00065   0.163   3.467e+02  4.924e-01  3.442e+00  4.055e-02     3.476e+02  2.657e-01  3.462e+00  4.145e-02 
00066   0.164   3.471e+02  2.650e-01  3.458e+00  4.137e-02     3.414e+02  1.799e-01  3.404e+00  6.072e-02 
00067   0.161   3.411e+02  1.792e-01  3.401e+00  6.061e-02     3.378e+02  2.093e-01  3.367e+00  8.774e-02 
00068   0.159   3.376e+02  2.083e-01  3.364e+00  8.757e-02     3.358e+02  2.802e-01  3.343e+00  1.180e-01 
00069   0.153   3.357e+02  2.787e-01  3.341e+00  1.177e-01     3.339e+02  3.604e-01  3.319e+00  1.515e-01 
00070   0.158   3.338e+02  3.583e-01  3.318e+00  1.513e-01     3.320e+02  4.399e-01  3.296e+00  1.847e-01 
00071   0.185   3.320e+02  4.373e-01  3.296e+00  1.844e-01     3.305e+02  5.113e-01  3.277e+00  2.097e-01 
00072   0.156   3.305e+02  5.084e-01  3.278e+00  2.093e-01     3.291e+02  5.634e-01  3.261e+00  2.179e-01 
00073   0.164   3.292e+02  5.603e-01  3.262e+00  2.176e-01     3.274e+02  5.845e-01  3.243e+00  2.058e-01 
00074   0.160   3.276e+02  5.815e-01  3.245e+00  2.055e-01     3.253e+02  5.706e-01  3.222e+00  1.776e-01 
00075   0.167   3.254e+02  5.678e-01  3.224e+00  1.774e-01     3.232e+02  5.288e-01  3.204e+00  1.429e-01 
resampling
00076   0.154   3.239e+02  5.285e-01  3.211e+00  1.434e-01     3.219e+02  4.744e-01  3.194e+00  1.108e-01 
00077   0.159   3.226e+02  4.742e-01  3.201e+00  1.113e-01     3.218e+02  4.223e-01  3.196e+00  8.567e-02 
00078   0.174   3.225e+02  4.222e-01  3.203e+00  8.610e-02     3.224e+02  3.829e-01  3.205e+00  6.845e-02 
00079   0.183   3.232e+02  3.830e-01  3.212e+00  6.883e-02     3.231e+02  3.617e-01  3.213e+00  5.840e-02 
00080   0.164   3.239e+02  3.619e-01  3.220e+00  5.875e-02     3.231e+02  3.603e-01  3.213e+00  5.455e-02 
00081   0.170   3.239e+02  3.605e-01  3.220e+00  5.490e-02     3.224e+02  3.779e-01  3.204e+00  5.608e-02 
00082   0.155   3.232e+02  3.782e-01  3.212e+00  5.646e-02     3.212e+02  4.117e-01  3.191e+00  6.213e-02 
00083   0.168   3.221e+02  4.119e-01  3.199e+00  6.256e-02     3.203e+02  4.549e-01  3.180e+00  7.116e-02 
00084   0.154   3.212e+02  4.550e-01  3.189e+00  7.167e-02     3.200e+02  4.967e-01  3.175e+00  8.044e-02 
00085   0.153   3.209e+02  4.967e-01  3.184e+00  8.102e-02     3.200e+02  5.247e-01  3.173e+00  8.631e-02 
00086   0.153   3.210e+02  5.247e-01  3.183e+00  8.697e-02     3.199e+02  5.302e-01  3.171e+00  8.583e-02 
00087   0.152   3.208e+02  5.301e-01  3.181e+00  8.653e-02     3.194e+02  5.122e-01  3.167e+00  7.862e-02 
00088   0.154   3.203e+02  5.120e-01  3.177e+00  7.930e-02     3.187e+02  4.770e-01  3.162e+00  6.714e-02 
00089   0.154   3.196e+02  4.768e-01  3.171e+00  6.778e-02     3.181e+02  4.348e-01  3.159e+00  5.501e-02 
00090   0.152   3.190e+02  4.347e-01  3.168e+00  5.558e-02     3.179e+02  3.960e-01  3.158e+00  4.500e-02 
00091   0.155   3.187e+02  3.959e-01  3.167e+00  4.551e-02     3.177e+02  3.684e-01  3.158e+00  3.836e-02 
00092   0.154   3.185e+02  3.683e-01  3.167e+00  3.883e-02     3.174e+02  3.565e-01  3.156e+00  3.527e-02 
00093   0.153   3.182e+02  3.564e-01  3.164e+00  3.572e-02     3.168e+02  3.617e-01  3.149e+00  3.542e-02 
00094   0.164   3.176e+02  3.617e-01  3.158e+00  3.588e-02     3.160e+02  3.823e-01  3.140e+00  3.830e-02 
00095   0.153   3.168e+02  3.823e-01  3.149e+00  3.881e-02     3.152e+02  4.124e-01  3.131e+00  4.310e-02 
00096   0.159   3.161e+02  4.124e-01  3.139e+00  4.369e-02     3.145e+02  4.424e-01  3.123e+00  4.845e-02 
00097   0.154   3.154e+02  4.424e-01  3.132e+00  4.913e-02     3.138e+02  4.620e-01  3.115e+00  5.258e-02 
00098   0.151   3.147e+02  4.621e-01  3.124e+00  5.335e-02     3.130e+02  4.647e-01  3.106e+00  5.421e-02 
00099   0.161   3.139e+02  4.649e-01  3.115e+00  5.505e-02     3.121e+02  4.528e-01  3.098e+00  5.357e-02 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_23_13_16_34
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.593   1173.773511   4.098e+00  1.105e+01  4.874e+01     1.088e+03  4.012e+00  1.026e+01  4.212e+01 
00002   0.158   1087.819580   4.001e+00  1.026e+01  4.210e+01     7.284e+02  2.510e+00  7.018e+00  1.399e+01 
00003   0.160   728.338883   2.502e+00  7.018e+00  1.398e+01     5.169e+02  1.460e+00  5.056e+00  3.977e+00 
00004   0.151   516.771937   1.456e+00  5.055e+00  3.973e+00     4.301e+02  9.755e-01  4.237e+00  1.535e+00 
00005   0.168   429.867375   9.715e-01  4.235e+00  1.531e+00     3.822e+02  7.074e-01  3.780e+00  6.926e-01 
00006   0.158   381.980323   7.035e-01  3.778e+00  6.884e-01     3.561e+02  5.557e-01  3.529e+00  3.828e-01 
00007   0.158   355.890328   5.520e-01  3.528e+00  3.795e-01     3.458e+02  4.776e-01  3.432e+00  2.593e-01 
00008   0.168   345.615808   4.743e-01  3.430e+00  2.566e-01     3.420e+02  3.967e-01  3.399e+00  1.971e-01 
00009   0.160   341.776043   3.938e-01  3.396e+00  1.947e-01     3.409e+02  3.173e-01  3.392e+00  1.720e-01 
00010   0.159   340.656445   3.147e-01  3.389e+00  1.699e-01     3.413e+02  2.929e-01  3.397e+00  1.791e-01 
00011   0.149   341.048349   2.901e-01  3.394e+00  1.770e-01     3.412e+02  3.233e-01  3.394e+00  2.078e-01 
00012   0.147   340.961241   3.204e-01  3.392e+00  2.056e-01     3.424e+02  3.891e-01  3.402e+00  2.610e-01 
00013   0.149   342.110177   3.862e-01  3.399e+00  2.585e-01     3.442e+02  4.356e-01  3.417e+00  3.469e-01 
00014   0.184   343.875151   4.328e-01  3.414e+00  3.439e-01     3.458e+02  4.474e-01  3.431e+00  4.511e-01 
00015   0.153   345.507616   4.444e-01  3.428e+00  4.476e-01     3.474e+02  4.595e-01  3.446e+00  5.389e-01 
00016   0.148   347.113565   4.561e-01  3.443e+00  5.348e-01     3.474e+02  4.811e-01  3.444e+00  5.845e-01 
00017   0.149   347.130968   4.774e-01  3.442e+00  5.800e-01     3.466e+02  5.046e-01  3.435e+00  5.895e-01 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_23_13_16_51
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.650   1118.546989   4.953e+00  1.027e+01  6.725e+01     1.147e+03  4.897e+00  1.061e+01  6.166e+01 
00002   0.174   1147.079199   4.894e+00  1.061e+01  6.167e+01     7.141e+02  2.905e+00  6.815e+00  1.805e+01 
00003   0.172   714.083913   2.904e+00  6.815e+00  1.806e+01     5.229e+02  1.932e+00  5.065e+00  6.803e+00 
00004   0.156   522.886916   1.932e+00  5.064e+00  6.808e+00     4.576e+02  1.567e+00  4.457e+00  4.077e+00 
00005   0.160   457.476210   1.568e+00  4.456e+00  4.081e+00     4.077e+02  1.262e+00  3.990e+00  2.449e+00 
00006   0.152   407.539560   1.263e+00  3.988e+00  2.450e+00     3.752e+02  9.929e-01  3.688e+00  1.457e+00 
00007   0.164   374.980686   9.927e-01  3.686e+00  1.456e+00     3.569e+02  7.569e-01  3.523e+00  8.185e-01 
00008   0.165   356.728966   7.565e-01  3.521e+00  8.172e-01     3.427e+02  5.678e-01  3.394e+00  4.631e-01 
00009   0.161   342.397891   5.679e-01  3.391e+00  4.622e-01     3.450e+02  5.346e-01  3.420e+00  3.189e-01 
00010   0.185   344.653351   5.353e-01  3.417e+00  3.184e-01     3.416e+02  3.977e-01  3.394e+00  2.722e-01 
00011   0.151   341.288067   3.982e-01  3.390e+00  2.717e-01     3.398e+02  4.062e-01  3.375e+00  2.812e-01 
00012   0.155   339.586370   4.059e-01  3.373e+00  2.805e-01     3.430e+02  4.587e-01  3.404e+00  2.931e-01 
00013   0.161   342.767237   4.581e-01  3.402e+00  2.923e-01     3.381e+02  3.267e-01  3.363e+00  2.595e-01 
00014   0.151   337.830674   3.269e-01  3.359e+00  2.590e-01     3.413e+02  3.904e-01  3.391e+00  2.655e-01 
00015   0.150   340.888304   3.910e-01  3.387e+00  2.652e-01     3.360e+02  3.808e-01  3.338e+00  2.904e-01 
00016   0.166   335.610893   3.810e-01  3.334e+00  2.900e-01     3.373e+02  3.881e-01  3.350e+00  3.297e-01 
00017   0.151   336.923073   3.877e-01  3.347e+00  3.292e-01     3.358e+02  4.700e-01  3.331e+00  3.749e-01 
00018   0.149   335.342095   4.695e-01  3.326e+00  3.745e-01     3.355e+02  3.960e-01  3.332e+00  3.747e-01 
00019   0.150   334.993330   3.961e-01  3.326e+00  3.746e-01     3.382e+02  5.048e-01  3.352e+00  4.424e-01 
00020   0.152   337.535643   5.047e-01  3.346e+00  4.424e-01     3.355e+02  4.596e-01  3.327e+00  4.619e-01 
00021   0.150   334.905363   4.593e-01  3.321e+00  4.619e-01     3.367e+02  5.278e-01  3.335e+00  4.991e-01 
00022   0.156   336.140200   5.271e-01  3.330e+00  4.989e-01     3.342e+02  4.678e-01  3.314e+00  4.633e-01 
00023   0.165   333.700708   4.675e-01  3.309e+00  4.634e-01     3.348e+02  4.291e-01  3.323e+00  4.185e-01 
00024   0.163   334.300289   4.293e-01  3.317e+00  4.188e-01     3.349e+02  4.179e-01  3.325e+00  3.825e-01 
00025   0.150   334.421212   4.182e-01  3.319e+00  3.828e-01     3.335e+02  3.875e-01  3.312e+00  3.484e-01 
resampling
00026   0.152   332.900020   3.874e-01  3.306e+00  3.486e-01     3.344e+02  4.129e-01  3.320e+00  3.350e-01 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_23_13_17_58
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.630   1038.4372   4.329e+00  9.707e+00  4.607e+01     1.047e+03  4.479e+00  9.700e+00  5.473e+01 
00002   0.169   1047.1106   4.473e+00  9.700e+00  5.470e+01     6.269e+02  2.419e+00  6.024e+00  1.233e+01 
00003   0.155   626.8653   2.416e+00  6.025e+00  1.232e+01     4.389e+02  1.360e+00  4.295e+00  2.555e+00 
00004   0.185   438.7882   1.359e+00  4.294e+00  2.556e+00     3.798e+02  9.073e-01  3.742e+00  1.033e+00 
00005   0.160   379.6210   9.066e-01  3.741e+00  1.033e+00     3.539e+02  5.697e-01  3.505e+00  5.679e-01 
00006   0.164   353.7400   5.691e-01  3.503e+00  5.682e-01     3.474e+02  3.954e-01  3.450e+00  3.790e-01 
00007   0.181   347.1869   3.950e-01  3.448e+00  3.797e-01     3.411e+02  3.320e-01  3.391e+00  2.816e-01 
00008   0.161   340.8330   3.318e-01  3.389e+00  2.824e-01     3.415e+02  4.113e-01  3.392e+00  2.673e-01 
00009   0.195   341.2391   4.113e-01  3.389e+00  2.681e-01     3.404e+02  4.217e-01  3.380e+00  3.286e-01 
00010   0.158   340.1849   4.218e-01  3.377e+00  3.295e-01     3.405e+02  4.061e-01  3.380e+00  4.485e-01 
00011   0.156   340.1930   4.061e-01  3.377e+00  4.497e-01     3.428e+02  4.809e-01  3.398e+00  5.847e-01 
00012   0.163   342.5997   4.807e-01  3.396e+00  5.862e-01     3.435e+02  5.884e-01  3.399e+00  6.794e-01 
00013   0.154   343.2927   5.881e-01  3.397e+00  6.809e-01     3.454e+02  6.461e-01  3.415e+00  7.020e-01 
00014   0.156   345.2396   6.461e-01  3.413e+00  7.036e-01     3.439e+02  5.625e-01  3.404e+00  6.520e-01 
00015   0.154   343.6207   5.627e-01  3.402e+00  6.536e-01     3.435e+02  4.747e-01  3.405e+00  5.574e-01 
00016   0.163   343.2224   4.747e-01  3.403e+00  5.589e-01     3.414e+02  4.362e-01  3.388e+00  4.593e-01 
00017   0.151   341.2390   4.359e-01  3.386e+00  4.605e-01     3.414e+02  4.451e-01  3.388e+00  3.834e-01 
00018   0.149   341.2287   4.447e-01  3.386e+00  3.842e-01     3.402e+02  3.982e-01  3.379e+00  3.344e-01 
00019   0.151   339.9571   3.981e-01  3.376e+00  3.351e-01     3.399e+02  3.410e-01  3.379e+00  3.108e-01 
00020   0.153   339.6535   3.411e-01  3.376e+00  3.116e-01     3.391e+02  3.311e-01  3.372e+00  3.045e-01 
00021   0.157   338.8457   3.311e-01  3.369e+00  3.052e-01     3.379e+02  3.734e-01  3.357e+00  3.130e-01 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_23_13_18_33
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.560   1238.2109   4.7807 11.5732 56.9870      1.082e+03  4.118e+00  1.019e+01  4.167e+01 
00002   0.163   1081.9134   4.1474 10.1932 41.8558      7.646e+02  2.816e+00  7.340e+00  1.645e+01 
00003   0.158   764.6939   2.8364 7.3398 16.5314      5.577e+02  1.859e+00  5.428e+00  5.577e+00 
00004   0.156   557.8067   1.8729 5.4283 5.6098      4.575e+02  1.297e+00  4.488e+00  2.251e+00 
00005   0.153   457.7509   1.3065 4.4895 2.2661      4.045e+02  9.162e-01  3.989e+00  1.054e+00 
00006   0.154   404.8244   0.9233 3.9915 1.0616      3.766e+02  6.592e-01  3.727e+00  5.704e-01 
00007   0.159   377.0152   0.6645 3.7312 0.5745      3.624e+02  5.095e-01  3.595e+00  3.681e-01 
00008   0.164   362.9026   0.5138 3.5996 0.3706      3.550e+02  4.460e-01  3.524e+00  2.837e-01 
00009   0.171   355.5115   0.4497 3.5298 0.2857      3.515e+02  4.365e-01  3.491e+00  2.472e-01 
00010   0.175   352.0634   0.4400 3.4961 0.2489      3.494e+02  4.259e-01  3.470e+00  2.332e-01 
00011   0.171   349.9560   0.4293 3.4757 0.2348      3.476e+02  4.053e-01  3.453e+00  2.431e-01 
00012   0.156   348.1548   0.4088 3.4587 0.2448      3.465e+02  4.077e-01  3.442e+00  2.833e-01 
00013   0.153   347.0098   0.4115 3.4467 0.2855      3.456e+02  4.499e-01  3.430e+00  3.535e-01 
00014   0.153   346.0256   0.4543 3.4340 0.3565      3.448e+02  5.078e-01  3.418e+00  4.153e-01 
00015   0.166   345.1683   0.5127 3.4219 0.4191      3.438e+02  5.309e-01  3.407e+00  4.223e-01 
00016   0.153   344.1561   0.5361 3.4105 0.4265      3.422e+02  4.986e-01  3.393e+00  3.864e-01 
00017   0.150   342.5541   0.5036 3.3965 0.3904      3.403e+02  4.445e-01  3.377e+00  3.424e-01 
00018   0.154   340.6383   0.4493 3.3805 0.3461      3.386e+02  4.136e-01  3.363e+00  2.978e-01 
00019   0.155   338.9959   0.4181 3.3660 0.3012      3.371e+02  4.028e-01  3.348e+00  2.463e-01 
00020   0.148   337.4601   0.4070 3.3518 0.2493      3.363e+02  3.856e-01  3.342e+00  2.010e-01 
00021   0.159   336.7231   0.3895 3.3457 0.2034      3.353e+02  3.453e-01  3.334e+00  1.774e-01 
00022   0.165   335.6546   0.3491 3.3373 0.1797      3.348e+02  3.304e-01  3.330e+00  1.722e-01 
00023   0.151   335.1596   0.3342 3.3331 0.1744      3.337e+02  3.580e-01  3.317e+00  1.737e-01 
00024   0.152   334.0482   0.3619 3.3206 0.1758      3.333e+02  3.973e-01  3.311e+00  1.810e-01 
00025   0.150   333.5965   0.4014 3.3141 0.1831      3.324e+02  3.924e-01  3.302e+00  1.941e-01 
resampling
00026   0.155   332.8880   0.3962 3.3071 0.1962      3.321e+02  4.021e-01  3.299e+00  2.034e-01 
00027   0.151   332.6091   0.4060 3.3037 0.2055      3.317e+02  4.456e-01  3.292e+00  2.047e-01 
00028   0.152   332.1544   0.4496 3.2970 0.2067      3.311e+02  4.464e-01  3.287e+00  1.967e-01 
00029   0.167   331.6217   0.4503 3.2917 0.1985      3.308e+02  4.149e-01  3.285e+00  1.761e-01 
00030   0.162   331.2761   0.4186 3.2901 0.1775      3.302e+02  4.179e-01  3.280e+00  1.529e-01 
00031   0.152   330.7184   0.4215 3.2846 0.1540      3.295e+02  4.089e-01  3.273e+00  1.296e-01 
00032   0.166   330.0036   0.4123 3.2781 0.1305      3.289e+02  3.854e-01  3.269e+00  1.095e-01 
00033   0.149   329.4505   0.3886 3.2740 0.1101      3.286e+02  3.925e-01  3.265e+00  9.662e-02 
00034   0.170   329.0557   0.3957 3.2698 0.0970      3.279e+02  3.902e-01  3.259e+00  8.358e-02 
00035   0.153   328.3677   0.3934 3.2632 0.0839      3.273e+02  4.143e-01  3.251e+00  7.618e-02 
00036   0.169   327.7271   0.4176 3.2556 0.0764      3.267e+02  4.087e-01  3.246e+00  7.010e-02 
00037   0.150   327.1717   0.4119 3.2504 0.0702      3.261e+02  4.136e-01  3.240e+00  6.171e-02 
00038   0.164   326.5196   0.4168 3.2437 0.0618      3.256e+02  4.391e-01  3.234e+00  5.644e-02 
00039   0.147   326.0374   0.4422 3.2377 0.0564      3.252e+02  3.894e-01  3.232e+00  4.923e-02 
00040   0.156   325.6451   0.3922 3.2363 0.0492      3.247e+02  4.391e-01  3.225e+00  5.092e-02 
00041   0.151   325.1979   0.4417 3.2294 0.0508      3.243e+02  3.923e-01  3.223e+00  4.249e-02 
00042   0.150   324.7576   0.3948 3.2274 0.0424      3.234e+02  4.427e-01  3.212e+00  5.790e-02 
00043   0.152   323.8856   0.4451 3.2160 0.0576      3.222e+02  4.040e-01  3.201e+00  5.154e-02 
00044   0.157   322.6129   0.4063 3.2053 0.0513      3.211e+02  4.498e-01  3.188e+00  5.909e-02 
00045   0.150   321.4851   0.4520 3.1917 0.0589      3.199e+02  4.224e-01  3.177e+00  6.526e-02 
00046   0.151   320.2811   0.4244 3.1809 0.0650      3.188e+02  4.021e-01  3.167e+00  6.340e-02 
00047   0.155   319.2085   0.4039 3.1713 0.0631      3.180e+02  4.525e-01  3.157e+00  7.590e-02 
00048   0.153   318.3979   0.4542 3.1605 0.0756      3.173e+02  3.969e-01  3.152e+00  7.235e-02 
00049   0.156   317.6322   0.3984 3.1557 0.0720      3.165e+02  5.126e-01  3.138e+00  1.086e-01 
00050   0.149   316.7531   0.5143 3.1407 0.1081      3.152e+02  4.364e-01  3.130e+00  8.886e-02 
resampling
00051   0.150   315.8027   0.4373 3.1353 0.0887      3.127e+02  4.756e-01  3.102e+00  1.366e-01 
00052   0.148   313.4018   0.4763 3.1088 0.1363      3.106e+02  4.458e-01  3.082e+00  1.386e-01 
00053   0.156   311.2529   0.4464 3.0888 0.1382      3.106e+02  4.643e-01  3.081e+00  1.568e-01 
00054   0.159   311.1350   0.4646 3.0866 0.1563      3.115e+02  5.117e-01  3.088e+00  2.057e-01 
00055   0.151   312.2908   0.5120 3.0953 0.2049      3.144e+02  4.733e-01  3.118e+00  2.061e-01 
00056   0.153   314.5757   0.4731 3.1200 0.2054      3.219e+02  6.479e-01  3.184e+00  2.475e-01 
00057   0.158   323.3064   0.6498 3.1981 0.2472      3.358e+02  6.403e-01  3.322e+00  3.736e-01 
00058   0.150   335.7156   0.6425 3.3213 0.3711      3.210e+02  5.374e-01  3.181e+00  2.458e-01 
00059   0.151   320.8800   0.5384 3.1794 0.2451      3.372e+02  7.115e-01  3.333e+00  2.653e-01 
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_23_13_19_47
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.528   1148.1834   4.2516 10.8411 42.8155     1065.8858   4.0994 10.0411 41.2776  
00002   0.178   1065.9741   4.1035 10.0411 41.3463     697.7946   2.5152 6.7216 13.0549  
00003   0.162   697.8349   2.5182 6.7216 13.0843     500.1606   1.4750 4.8892 3.8640  
00004   0.159   500.1698   1.4771 4.8891 3.8747     419.7984   0.9617 4.1344 1.5512  
00005   0.154   419.7985   0.9632 4.1343 1.5558     376.7967   0.6798 3.7270 0.6941  
00006   0.154   376.7857   0.6809 3.7269 0.6960     355.3825   0.5053 3.5249 0.3690  
00007   0.164   355.3620   0.5061 3.5246 0.3698     346.0183   0.3552 3.4399 0.2491  
00008   0.162   346.0142   0.3559 3.4399 0.2497     342.3907   0.3157 3.4058 0.2275  
00009   0.162   342.4000   0.3162 3.4059 0.2280     341.3636   0.3790 3.3920 0.2647  
00010   0.160   341.3566   0.3794 3.3919 0.2651     340.5289   0.3978 3.3820 0.3443  
00011   0.152   340.5206   0.3983 3.3818 0.3449     341.9341   0.4300 3.3930 0.4813  
00012   0.162   341.9302   0.4306 3.3929 0.4823     343.0455   0.5166 3.3982 0.6415  
00013   0.155   343.0222   0.5173 3.3979 0.6427     345.1898   0.5870 3.4148 0.7767  
00014   0.155   345.1519   0.5879 3.4143 0.7782     345.2937   0.5583 3.4166 0.8411  
00015   0.159   345.2642   0.5592 3.4163 0.8429     345.0845   0.5240 3.4164 0.8229  
00016   0.150   345.0573   0.5249 3.4161 0.8248     343.8122   0.5090 3.4052 0.7465  
00017   0.149   343.7751   0.5099 3.4048 0.7482     342.9650   0.4669 3.3997 0.6628  
00018   0.151   342.9310   0.4677 3.3993 0.6643     342.1460   0.3964 3.3957 0.5931  
00019   0.151   342.1241   0.3972 3.3954 0.5945     341.5275   0.3702 3.3913 0.5454  
00020   0.156   341.5066   0.3709 3.3911 0.5467     341.0193   0.3810 3.3859 0.5251  
00021   0.158   340.9922   0.3816 3.3856 0.5262     340.1510   0.3711 3.3776 0.5310  
00022   0.152   340.1244   0.3717 3.3773 0.5321     339.4986   0.3640 3.3712 0.5608  
00023   0.155   339.4754   0.3646 3.3709 0.5619     338.5649   0.3931 3.3599 0.6067  
00024   0.152   338.5344   0.3937 3.3596 0.6078     338.1023   0.4378 3.3525 0.6627  
00025   0.161   338.0587   0.4384 3.3520 0.6638     337.5294   0.4512 3.3456 0.7153  
resampling
00026   0.161   337.2497   0.4494 3.3429 0.7125     337.3025   0.4600 3.3425 0.7491  
00027   0.168   337.0507   0.4581 3.3401 0.7462     336.8100   0.4799 3.3366 0.7513  
00028   0.168   336.5494   0.4780 3.3341 0.7484     336.3782   0.4857 3.3323 0.7223  
00029   0.159   336.1083   0.4838 3.3297 0.7194     335.6629   0.4567 3.3271 0.6696  
00030   0.161   335.4093   0.4547 3.3247 0.6669     335.1153   0.4308 3.3235 0.6070  
00031   0.158   334.8663   0.4289 3.3212 0.6046     334.5568   0.4227 3.3190 0.5469  
00032   0.152   334.2867   0.4209 3.3164 0.5446     334.1342   0.4100 3.3159 0.4966  
00033   0.161   333.8585   0.4083 3.3132 0.4946     333.6159   0.3893 3.3121 0.4574  
00034   0.154   333.3579   0.3876 3.3096 0.4556     333.0759   0.3903 3.3069 0.4308  
00035   0.149   332.8232   0.3885 3.3045 0.4292     332.4633   0.4098 3.3000 0.4158  
00036   0.150   332.2073   0.4080 3.2975 0.4141     331.8304   0.4223 3.2931 0.4052  
00037   0.151   331.5930   0.4205 3.2909 0.4036     331.2797   0.4322 3.2873 0.3930  
00038   0.149   331.0685   0.4302 3.2853 0.3917     330.7583   0.4515 3.2812 0.3795  
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_23_13_20_09
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.564    990.5548   4.5105    9.2063      47.3677     950.2891    s4.3247    8.8535      43.3161  
00002   0.164    950.1713   4.3098    8.8542      43.1982     615.4085    s2.5267    5.9042      12.3532  
00003   0.173    615.3470   2.5170    5.9045      12.3071     437.2302    s1.3568    4.2752      2.9284  
00004   0.171    437.1238   1.3512    4.2745      2.9148     383.2223    s0.9050    3.7754      1.1581  
00005   0.153    383.0339   0.9012    3.7738      1.1518     359.2171    s0.7005    3.5514      0.5760  
00006   0.163    358.9616   0.6977    3.5490      0.5727     346.2002    s0.5315    3.4323      0.3153  
00007   0.163    345.8741   0.5294    3.4291      0.3136     340.1463    s0.3868    3.3801      0.1989  
00008   0.163    339.7528   0.3851    3.3763      0.1979     337.8066    s0.3371    3.3596      0.1612  
00009   0.165    337.3857   0.3356    3.3555      0.1604     337.0505    s0.3932    3.3490      0.1801  
00010   0.160    336.6521   0.3916    3.3451      0.1791     336.9839    s0.4316    3.3460      0.2254  
00011   0.150    336.6061   0.4299    3.3423      0.2241     337.9085    s0.4346    3.3544      0.2945  
00012   0.156    337.5307   0.4328    3.3507      0.2929     339.2701    s0.4867    3.3645      0.3852  
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_24_09_43_14
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   2.504    1041.1618   4.3013    9.7702      42.6394     879.2488    s3.5874    8.3205      29.2618  
00002   0.168    879.2881   3.5878    8.3205      29.2956     626.3589    s2.3473    6.0384      10.7845  
00003   0.169    626.3700   2.3484    6.0382      10.8069     459.8693    s1.3693    4.4998      3.0404  
00004   0.168    459.8436   1.3703    4.4994      3.0497     386.0471    s0.8547    3.8076      1.0169  
00005   0.166    386.0277   0.8553    3.8073      1.0208     357.1322    s0.6093    3.5367      0.4192  
00006   0.165    357.1348   0.6096    3.5367      0.4211     344.5787    s0.4108    3.4230      0.2218  
00007   0.173    344.5432   0.4112    3.4226      0.2229     341.2484    s0.2919    3.3963      0.1633  
00008   0.174    341.1513   0.2925    3.3952      0.1641     340.4544    s0.2973    3.3881      0.1547  
00009   0.170    340.3728   0.2979    3.3873      0.1554     340.0389    s0.3521    3.3810      0.1776  
00010   0.163    340.0010   0.3526    3.3806      0.1784     339.8942    s0.3708    3.3781      0.2272  
00011   0.162    339.8367   0.3712    3.3775      0.2280     341.5456    s0.4214    3.3914      0.3002  
00012   0.161    341.4620   0.4220    3.3905      0.3013     342.6510    s0.5083    3.3972      0.3854  
00013   0.163    342.5841   0.5089    3.3965      0.3868     344.5659    s0.5578    3.4134      0.4355  
00014   0.164    344.5076   0.5585    3.4128      0.4372     343.8268    s0.5056    3.4089      0.4138  
00015   0.162    343.7462   0.5062    3.4080      0.4153     343.0453    s0.4588    3.4039      0.3579  
00016   0.164    342.9693   0.4593    3.4031      0.3592     341.4898    s0.4397    3.3899      0.3060  
00017   0.163    341.4465   0.4401    3.3894      0.3072     340.2457    s0.4058    3.3795      0.2680  
00018   0.162    340.1966   0.4063    3.3790      0.2691     339.6493    s0.3553    3.3763      0.2426  
00019   0.161    339.5600   0.3559    3.3754      0.2437     338.9425    s0.3383    3.3702      0.2292  
00020   0.161    338.8489   0.3389    3.3692      0.2302     338.3720    s0.3616    3.3634      0.2289  
00021   0.162    338.3047   0.3620    3.3626      0.2298     337.7339    s0.3720    3.3564      0.2379  
00022   0.163    337.6600   0.3725    3.3556      0.2389     336.7806    s0.3623    3.3472      0.2528  
00023   0.162    336.6671   0.3629    3.3460      0.2538     336.1037    s0.3781    3.3394      0.2712  
00024   0.162    335.9751   0.3788    3.3381      0.2723     335.2880    s0.4121    3.3294      0.2872  
00025   0.163    335.1772   0.4128    3.3282      0.2883     334.8483    s0.4305    3.3240      0.2997  
resampling
00026   0.164    335.7000   0.4319    3.3324      0.3010     334.5612    s0.4308    3.3210      0.3084  
00027   0.161    335.4046   0.4323    3.3293      0.3097     334.0254    s0.4408    3.3151      0.3114  
00028   0.161    334.8826   0.4423    3.3236      0.3127     333.5227    s0.4589    3.3092      0.3048  
00029   0.162    334.4054   0.4603    3.3180      0.3060     332.7641    s0.4522    3.3022      0.2845  
00030   0.160    333.6490   0.4535    3.3110      0.2855     332.0916    s0.4308    3.2968      0.2553  
00031   0.162    332.9654   0.4321    3.3055      0.2561     331.4911    s0.4198    3.2917      0.2256  
00032   0.162    332.3745   0.4210    3.3004      0.2263     330.9364    s0.4128    3.2867      0.1991  
00033   0.160    331.8435   0.4139    3.2957      0.1996     330.5039    s0.3944    3.2836      0.1758  
00034   0.162    331.4209   0.3955    3.2927      0.1763     330.0721    s0.3774    3.2803      0.1569  
00035   0.160    330.9873   0.3784    3.2894      0.1572     329.5385    s0.3827    3.2748      0.1441  
00036   0.162    330.4598   0.3836    3.2840      0.1444     329.0414    s0.4028    3.2689      0.1370  
00037   0.162    329.9742   0.4037    3.2782      0.1372     328.5194    s0.4140    3.2632      0.1324  
00038   0.158    329.4533   0.4149    3.2725      0.1326     328.1383    s0.4187    3.2592      0.1282  
00039   0.162    329.0677   0.4195    3.2684      0.1282     327.8363    s0.4302    3.2556      0.1223  
00040   0.162    328.7724   0.4311    3.2649      0.1223     327.5285    s0.4416    3.2521      0.1131  
00041   0.168    328.4793   0.4424    3.2615      0.1131     327.2468    s0.4365    3.2496      0.1007  
00042   0.168    328.2051   0.4372    3.2592      0.1007     326.9744    s0.4194    3.2479      0.0879  
00043   0.165    327.9366   0.4201    3.2575      0.0878     326.7019    s0.4084    3.2458      0.0772  
00044   0.162    327.6773   0.4091    3.2555      0.0771     326.5095    s0.4069    3.2441      0.0695  
00045   0.162    327.5001   0.4075    3.2539      0.0694     326.3414    s0.4051    3.2425      0.0638  
00046   0.163    327.3343   0.4057    3.2524      0.0637     326.2279    s0.4051    3.2414      0.0600  
00047   0.165    327.2166   0.4056    3.2513      0.0598     326.1231    s0.4120    3.2401      0.0575  
00048   0.165    327.1187   0.4124    3.2500      0.0573     325.9939    s0.4180    3.2385      0.0553  
00049   0.160    327.0064   0.4185    3.2486      0.0551     325.8754    s0.4137    3.2375      0.0527  
00050   0.159    326.9011   0.4141    3.2478      0.0525     325.8023    s0.4074    3.2372      0.0501  
resampling
00051   0.163    326.8453   0.4085    3.2475      0.0499     325.7427    s0.4178    3.2360      0.0489  
00052   0.158    326.7850   0.4189    3.2464      0.0487     325.7361    s0.4275    3.2355      0.0484  
00053   0.161    326.7790   0.4286    3.2459      0.0483     325.6885    s0.4247    3.2352      0.0477  
00054   0.150    326.7498   0.4257    3.2457      0.0475     325.6580    s0.4231    3.2350      0.0463  
00055   0.145    326.7274   0.4241    3.2456      0.0461     325.6271    s0.4256    3.2345      0.0442  
00056   0.150    326.6915   0.4266    3.2451      0.0440     325.5810    s0.4196    3.2344      0.0415  
00057   0.146    326.6565   0.4205    3.2451      0.0413     325.5461    s0.4108    3.2345      0.0395  
00058   0.153    326.6396   0.4117    3.2454      0.0394     325.5203    s0.4143    3.2341      0.0391  
00059   0.145    326.6075   0.4152    3.2449      0.0389     325.5133    s0.4226    3.2336      0.0395  
00060   0.145    326.5850   0.4235    3.2443      0.0393     325.4661    s0.4204    3.2332      0.0395  
00061   0.155    326.5431   0.4213    3.2440      0.0393     325.4146    s0.4160    3.2330      0.0391  
00062   0.149    326.5019   0.4169    3.2438      0.0389     325.3636    s0.4186    3.2323      0.0387  
00063   0.150    326.4469   0.4195    3.2431      0.0386     325.3208    s0.4192    3.2319      0.0389  
00064   0.147    326.4038   0.4201    3.2426      0.0387     325.2645    s0.4163    3.2314      0.0397  
00065   0.158    326.3536   0.4172    3.2423      0.0395     325.2237    s0.4208    3.2308      0.0410  
00066   0.147    326.3038   0.4217    3.2415      0.0408     325.1866    s0.4268    3.2301      0.0421  
00067   0.156    326.2537   0.4277    3.2407      0.0419     325.1160    s0.4224    3.2296      0.0423  
00068   0.146    326.1876   0.4233    3.2403      0.0421     325.0505    s0.4173    3.2292      0.0423  
00069   0.151    326.1252   0.4183    3.2399      0.0421     324.9901    s0.4195    3.2285      0.0431  
00070   0.147    326.0527   0.4204    3.2391      0.0429     324.9221    s0.4201    3.2278      0.0443  
00071   0.146    325.9778   0.4210    3.2383      0.0441     324.8464    s0.4190    3.2271      0.0458  
00072   0.171    325.9008   0.4200    3.2376      0.0456     324.7676    s0.4223    3.2261      0.0475  
00073   0.153    325.8120   0.4233    3.2365      0.0473     324.6765    s0.4219    3.2252      0.0487  
00074   0.147    325.7172   0.4229    3.2355      0.0485     324.5646    s0.4168    3.2243      0.0498  
00075   0.149    325.6109   0.4178    3.2347      0.0496     324.4503    s0.4192    3.2230      0.0519  
resampling
00076   0.149    324.4491   0.4199    3.2230      0.0517     324.3186    s0.4207    3.2216      0.0540  
00077   0.146    324.3172   0.4214    3.2216      0.0538     324.1645    s0.4204    3.2201      0.0557  
00078   0.153    324.1633   0.4211    3.2200      0.0556     323.9845    s0.4218    3.2182      0.0572  
00079   0.148    323.9796   0.4224    3.2181      0.0571     323.7685    s0.4210    3.2161      0.0584  
00080   0.146    323.7609   0.4216    3.2159      0.0582     323.5077    s0.4199    3.2135      0.0595  
00081   0.146    323.4976   0.4205    3.2134      0.0593     323.1917    s0.4224    3.2102      0.0608  
00082   0.147    323.1760   0.4230    3.2100      0.0606     322.7987    s0.4225    3.2062      0.0620  
00083   0.147    322.7803   0.4231    3.2060      0.0618     322.3111    s0.4229    3.2013      0.0630  
00084   0.147    322.2906   0.4234    3.2011      0.0629     321.7067    s0.4248    3.1952      0.0641  
00085   0.146    321.6842   0.4254    3.1949      0.0639     320.9606    s0.4252    3.1877      0.0653  
00086   0.147    320.9395   0.4257    3.1875      0.0651     320.0722    s0.4314    3.1785      0.0668  
00087   0.146    320.0472   0.4319    3.1782      0.0666     319.0571    s0.4335    3.1682      0.0682  
00088   0.147    319.0333   0.4338    3.1680      0.0679     318.0375    s0.4458    3.1574      0.0696  
00089   0.146    318.0095   0.4461    3.1571      0.0693     317.2044    s0.4444    3.1491      0.0713  
00090   0.145    317.1844   0.4447    3.1489      0.0709     317.1613    s0.4963    3.1461      0.0745  
00091   0.146    317.1173   0.4967    3.1456      0.0740     322.0060    s0.4571    3.1963      0.0876  
00092   0.146    321.9840   0.4576    3.1961      0.0867     338.0395    s0.8039    3.3391      0.1080  
00093   0.148    337.9555   0.8047    3.3382      0.1076     328.4225    s0.5599    3.2551      0.1115  
00094   0.147    328.1839   0.5607    3.2527      0.1100     322.3952    s0.3619    3.2052      0.0686  
00095   0.149    322.3785   0.3624    3.2050      0.0677     324.7078    s0.3622    3.2285      0.0418  
00096   0.146    324.8482   0.3629    3.2299      0.0415     322.3077    s0.4964    3.1978      0.0483  
00097   0.147    322.2789   0.4974    3.1974      0.0479     322.8165    s0.5686    3.1989      0.0801  
00098   0.151    322.5833   0.5698    3.1965      0.0793     320.7404    s0.4923    3.1820      0.0780  
00099   0.148    320.5948   0.4930    3.1805      0.0771     317.4971    s0.3523    3.1568      0.0565  
00100   0.145    317.6016   0.3528    3.1578      0.0560     320.3449    s0.3661    3.1846      0.0518  
resampling
00101   0.149    321.0954   0.3651    3.1922      0.0520     316.8916    s0.4632    3.1451      0.0635  
00102   0.171    317.6748   0.4622    3.1530      0.0637     316.6585    s0.5766    3.1368      0.0908  
00103   0.161    317.3951   0.5752    3.1443      0.0910     318.6997    s0.6006    3.1560      0.0999  
00104   0.167    319.4370   0.5990    3.1634      0.1002     316.0469    s0.4995    3.1348      0.0671  
00105   0.173    316.8704   0.4981    3.1431      0.0674     315.6603    s0.3891    3.1367      0.0404  
00106   0.170    316.5331   0.3881    3.1455      0.0407     316.6247    s0.3572    3.1481      0.0334  
00107   0.170    317.4615   0.3562    3.1565      0.0336     315.6110    s0.3957    3.1359      0.0389  
00108   0.155    316.3862   0.3946    3.1437      0.0390     314.9104    s0.4652    3.1253      0.0516  
00109   0.184    315.6430   0.4638    3.1327      0.0518     314.9129    s0.5117    3.1229      0.0627  
00110   0.168    315.6534   0.5102    3.1304      0.0630     313.7610    s0.5021    3.1119      0.0626  
00111   0.167    314.5900   0.5005    3.1202      0.0630     313.0579    s0.4585    3.1071      0.0568  
00112   0.170    314.0315   0.4570    3.1169      0.0571     313.1254    s0.4245    3.1095      0.0549  
00113   0.148    314.2006   0.4231    3.1203      0.0552     312.0223    s0.4320    3.0980      0.0624  
00114   0.180    313.0697   0.4304    3.1086      0.0627     311.5995    s0.4847    3.0910      0.0805  
00115   0.166    312.5414   0.4826    3.1005      0.0808     312.0807    s0.5293    3.0934      0.0966  
00116   0.175    312.9626   0.5271    3.1023      0.0969     310.8294    s0.5131    3.0817      0.0975  
00117   0.157    311.7817   0.5108    3.0913      0.0977     310.3253    s0.4639    3.0791      0.0911  
00118   0.164    311.4378   0.4618    3.0904      0.0914     310.4550    s0.4364    3.0818      0.0905  
00119   0.155    311.6298   0.4343    3.0937      0.0907     309.7364    s0.4485    3.0739      0.1006  
00120   0.152    310.8007   0.4461    3.0847      0.1008     309.9099    s0.4726    3.0743      0.1143  
00121   0.161    310.8715   0.4700    3.0841      0.1145     309.0339    s0.4660    3.0658      0.1212  
00122   0.147    310.0631   0.4635    3.0762      0.1213     308.8323    s0.4543    3.0644      0.1250  
00123   0.146    310.0326   0.4519    3.0765      0.1249     308.1261    s0.4706    3.0564      0.1370  
00124   0.146    309.3213   0.4681    3.0684      0.1368     307.7325    s0.4996    3.0508      0.1529  
00125   0.147    308.7600   0.4967    3.0612      0.1528     307.4641    s0.4984    3.0481      0.1581  
resampling
00126   0.150    308.4366   0.4951    3.0580      0.1571     306.5165    s0.4766    3.0398      0.1557  
00127   0.156    307.6419   0.4738    3.0512      0.1546     306.0524    s0.4851    3.0346      0.1628  
00128   0.146    307.1028   0.4825    3.0453      0.1618     305.7699    s0.5037    3.0308      0.1729  
00129   0.146    306.5641   0.5009    3.0389      0.1722     305.3714    s0.4944    3.0273      0.1742  
00130   0.146    306.0981   0.4915    3.0347      0.1738     304.3798    s0.4720    3.0185      0.1720  
00131   0.146    305.2363   0.4691    3.0272      0.1715     303.9164    s0.4763    3.0136      0.1774  
00132   0.152    304.8383   0.4734    3.0229      0.1770     303.0861    s0.4968    3.0041      0.1913  
00133   0.147    303.8393   0.4937    3.0118      0.1914     302.4439    s0.5057    2.9971      0.2034  
00134   0.145    303.0564   0.5025    3.0034      0.2038     301.4022    s0.5013    2.9868      0.2108  
00135   0.150    302.0298   0.4981    2.9933      0.2110     300.4062    s0.5085    2.9764      0.2254  
00136   0.147    300.9835   0.5052    2.9823      0.2258     299.3759    s0.5115    2.9657      0.2478  
00137   0.153    299.7438   0.5081    2.9695      0.2493     298.0884    s0.4877    2.9540      0.2528  
00138   0.145    298.4148   0.4842    2.9574      0.2541     297.0832    s0.5026    2.9430      0.2720  
00139   0.145    297.1951   0.4992    2.9443      0.2739     296.8160    s0.5139    2.9395      0.2970  
00140   0.147    296.5868   0.5108    2.9373      0.3007     298.4300    s0.5120    2.9560      0.2676  
00141   0.146    298.0596   0.5072    2.9526      0.2677     310.4759    s0.6475    3.0694      0.3016  
00142   0.147    310.0741   0.6463    3.0653      0.3081     297.4022    s0.5109    2.9458      0.2679  
00143   0.145    296.7030   0.5066    2.9390      0.2712     313.7949    s0.4860    3.1115      0.2107  
00144   0.145    313.8135   0.4820    3.1119      0.2086     326.3942    s0.8147    3.2212      0.2004  
00145   0.145    326.3503   0.8154    3.2207      0.2050     331.1271    s0.7301    3.2734      0.1377  
00146   0.149    331.2863   0.7304    3.2749      0.1401     311.8022    s0.4614    3.0936      0.1354  
00147   0.145    312.0318   0.4601    3.0959      0.1374     306.4825    s0.3320    3.0472      0.1037  
00148   0.145    306.7592   0.3297    3.0501      0.1050     318.4134    s0.4272    3.1619      0.0842  
00149   0.145    318.9743   0.4258    3.1676      0.0847     309.0585    s0.4387    3.0680      0.0632  
00150   0.146    309.3849   0.4367    3.0714      0.0638     304.4736    s0.4827    3.0199      0.0682  
resampling
00151   0.157    304.1278   0.4819    3.0165      0.0694     306.9059    s0.5544    3.0406      0.0782  
00152   0.149    306.7099   0.5539    3.0386      0.0793     309.1350    s0.5852    3.0612      0.0842  
00153   0.145    309.0178   0.5846    3.0601      0.0853     308.8035    s0.5492    3.0597      0.0830  
00154   0.145    308.7498   0.5483    3.0592      0.0840     307.0053    s0.4728    3.0456      0.0764  
00155   0.146    307.0124   0.4717    3.0458      0.0774     305.5390    s0.4084    3.0343      0.0685  
00156   0.145    305.6146   0.4070    3.0351      0.0691     305.2756    s0.3943    3.0324      0.0635  
00157   0.153    305.4282   0.3928    3.0340      0.0637     305.2141    s0.4380    3.0296      0.0673  
00158   0.146    305.3640   0.4363    3.0312      0.0672     303.9580    s0.5228    3.0126      0.0832  
00159   0.146    303.8909   0.5207    3.0120      0.0833     303.8185    s0.6088    3.0067      0.1092  
00160   0.147    303.4858   0.6066    3.0034      0.1099     303.1982    s0.6126    3.0001      0.1248  
00161   0.157    302.8116   0.6107    2.9963      0.1259     300.6386    s0.5277    2.9788      0.1236  
00162   0.146    300.3430   0.5261    2.9759      0.1247     299.4819    s0.4360    2.9718      0.1178  
00163   0.148    299.3166   0.4344    2.9703      0.1186     299.6505    s0.3942    2.9757      0.1145  
00164   0.146    299.5795   0.3926    2.9750      0.1151     298.3029    s0.4237    2.9606      0.1195  
00165   0.158    298.1935   0.4222    2.9596      0.1203     296.3742    s0.5183    2.9364      0.1400  
00166   0.146    296.0621   0.5168    2.9334      0.1410     297.0218    s0.6053    2.9383      0.1677  
00167   0.146    296.4506   0.6037    2.9326      0.1688     297.4847    s0.5988    2.9431      0.1794  
00168   0.146    296.7942   0.5971    2.9363      0.1804     295.4336    s0.5128    2.9270      0.1674  
00169   0.146    294.8583   0.5111    2.9213      0.1682     295.0669    s0.4359    2.9273      0.1571  
00170   0.147    294.6639   0.4346    2.9233      0.1581     294.6553    s0.4288    2.9234      0.1734  
00171   0.146    294.1421   0.4275    2.9183      0.1752     294.6649    s0.4840    2.9205      0.1976  
00172   0.149    293.9564   0.4826    2.9134      0.2004     293.9093    s0.5372    2.9102      0.2074  
00173   0.146    292.9132   0.5348    2.9003      0.2099     294.3885    s0.5474    2.9145      0.2032  
00174   0.147    293.2830   0.5439    2.9036      0.2041     293.7230    s0.5102    2.9096      0.2105  
00175   0.147    292.7113   0.5074    2.8996      0.2122     293.9417    s0.5033    2.9120      0.2204  
resampling
00176   0.151    293.2059   0.5000    2.9048      0.2213     294.0962    s0.5185    2.9129      0.2137  
00177   0.147    293.3476   0.5150    2.9056      0.2143     293.5519    s0.5216    2.9074      0.1991  
00178   0.147    292.8259   0.5179    2.9004      0.1991     293.7277    s0.5193    2.9094      0.1882  
00179   0.147    292.9648   0.5156    2.9020      0.1885     293.2023    s0.5191    2.9042      0.1817  
00180   0.148    292.2613   0.5153    2.8950      0.1820     293.4498    s0.5077    2.9074      0.1757  
00181   0.148    292.4987   0.5040    2.8980      0.1758     292.7817    s0.4643    2.9029      0.1683  
00182   0.147    292.0031   0.4610    2.8953      0.1685     292.4220    s0.4500    2.9001      0.1643  
00183   0.147    291.8006   0.4468    2.8940      0.1645     291.9824    s0.4884    2.8937      0.1684  
00184   0.147    291.3248   0.4849    2.8873      0.1684     291.8685    s0.5364    2.8901      0.1798  
00185   0.151    291.0868   0.5326    2.8824      0.1797     291.5135    s0.5335    2.8866      0.1870  
00186   0.151    290.7483   0.5299    2.8791      0.1872     291.1938    s0.4998    2.8851      0.1860  
00187   0.152    290.5953   0.4963    2.8793      0.1861     291.0227    s0.4849    2.8841      0.1849  
00188   0.150    290.5249   0.4813    2.8793      0.1845     290.8247    s0.5045    2.8811      0.1956  
00189   0.148    290.1939   0.5005    2.8750      0.1953     291.0807    s0.5226    2.8826      0.2060  
00190   0.149    290.3136   0.5183    2.8752      0.2061     290.6740    s0.5141    2.8790      0.2058  
00191   0.146    289.9563   0.5096    2.8720      0.2054     290.5001    s0.5020    2.8779      0.2051  
00192   0.148    289.9005   0.4976    2.8721      0.2041     290.2760    s0.5126    2.8750      0.2165  
00193   0.154    289.6078   0.5085    2.8685      0.2161     290.1547    s0.5321    2.8727      0.2228  
00194   0.146    289.4414   0.5280    2.8658      0.2227     289.8074    s0.5290    2.8694      0.2180  
00195   0.148    289.1961   0.5248    2.8635      0.2172     289.5873    s0.5122    2.8681      0.2190  
00196   0.158    288.9812   0.5080    2.8622      0.2184     289.4109    s0.5146    2.8661      0.2275  
00197   0.146    288.6863   0.5102    2.8591      0.2270     289.3656    s0.5147    2.8656      0.2310  
00198   0.147    288.6228   0.5101    2.8584      0.2301     289.0746    s0.5008    2.8634      0.2297  
00199   0.178    288.3910   0.4963    2.8568      0.2290     288.9005    s0.5169    2.8608      0.2339  
00200   0.145    288.1972   0.5123    2.8540      0.2331     288.8711    s0.5319    2.8597      0.2420  
resampling
00201   0.151    288.6272   0.5275    2.8575      0.2424     288.6726    s0.5228    2.8582      0.2427  
00202   0.156    288.4458   0.5183    2.8561      0.2428     288.5590    s0.5100    2.8577      0.2419  
00203   0.156    288.3693   0.5055    2.8560      0.2416     288.4893    s0.5103    2.8569      0.2433  
00204   0.149    288.2608   0.5057    2.8549      0.2431     288.4196    s0.5213    2.8557      0.2470  
00205   0.149    288.0986   0.5168    2.8527      0.2471     288.3932    s0.5261    2.8551      0.2498  
00206   0.147    288.0359   0.5216    2.8518      0.2502     288.2488    s0.5172    2.8541      0.2514  
00207   0.149    287.9358   0.5128    2.8512      0.2517     288.1211    s0.5062    2.8534      0.2534  
00208   0.149    287.8672   0.5018    2.8510      0.2534     288.0250    s0.5064    2.8524      0.2574  
00209   0.165    287.7719   0.5019    2.8501      0.2573     287.9634    s0.5165    2.8512      0.2630  
00210   0.152    287.6717   0.5120    2.8485      0.2630     287.8805    s0.5246    2.8499      0.2678  
00211   0.161    287.5865   0.5201    2.8472      0.2678     287.6974    s0.5249    2.8480      0.2705  
00212   0.147    287.4648   0.5203    2.8459      0.2703     287.5525    s0.5229    2.8467      0.2728  
00213   0.149    287.3992   0.5182    2.8454      0.2721     287.4431    s0.5246    2.8454      0.2771  
00214   0.148    287.3136   0.5198    2.8444      0.2764     287.3812    s0.5284    2.8446      0.2826  
00215   0.147    287.2264   0.5236    2.8433      0.2820     287.3180    s0.5288    2.8439      0.2861  
00216   0.152    287.1528   0.5240    2.8425      0.2855     287.1989    s0.5254    2.8429      0.2870  
00217   0.150    287.0559   0.5204    2.8417      0.2861     287.1026    s0.5236    2.8420      0.2875  
00218   0.153    286.9678   0.5185    2.8409      0.2865     287.0336    s0.5259    2.8412      0.2890  
00219   0.160    286.8623   0.5209    2.8397      0.2883     286.9901    s0.5275    2.8406      0.2903  
00220   0.155    286.7777   0.5226    2.8387      0.2898     286.9056    s0.5240    2.8400      0.2905  
00221   0.152    286.6792   0.5190    2.8379      0.2899     286.8355    s0.5195    2.8395      0.2906  
00222   0.149    286.6000   0.5145    2.8374      0.2899     286.7900    s0.5212    2.8389      0.2918  
00223   0.148    286.5153   0.5162    2.8364      0.2913     286.7438    s0.5267    2.8382      0.2940  
00224   0.156    286.4244   0.5217    2.8352      0.2936     286.6583    s0.5267    2.8373      0.2965  
00225   0.158    286.3399   0.5216    2.8344      0.2961     286.5264    s0.5207    2.8362      0.2992  
resampling
00226   0.162    285.3822   0.5171    2.8250      0.2992     286.4598    s0.5105    2.8361      0.3009  
00227   0.150    285.2493   0.5066    2.8242      0.3007     286.3857    s0.5161    2.8350      0.3061  
00228   0.150    285.1407   0.5120    2.8227      0.3059     286.2296    s0.5326    2.8325      0.3146  
00229   0.154    285.0217   0.5286    2.8206      0.3144     286.1458    s0.5371    2.8314      0.3206  
00230   0.155    284.9741   0.5332    2.8199      0.3204     286.0480    s0.5266    2.8309      0.3218  
00231   0.155    284.8695   0.5226    2.8193      0.3214     286.0092    s0.5258    2.8306      0.3218  
00232   0.149    284.7992   0.5217    2.8187      0.3213     285.8966    s0.5398    2.8287      0.3229  
00233   0.160    284.6673   0.5357    2.8167      0.3224     285.8198    s0.5419    2.8279      0.3227  
00234   0.152    284.5957   0.5379    2.8158      0.3222     285.6807    s0.5242    2.8274      0.3205  
00235   0.158    284.4730   0.5202    2.8155      0.3201     285.6309    s0.5166    2.8273      0.3191  
00236   0.148    284.4089   0.5125    2.8153      0.3187     285.5555    s0.5293    2.8259      0.3200  
00237   0.165    284.2949   0.5251    2.8135      0.3195     285.4847    s0.5340    2.8249      0.3208  
00238   0.150    284.2262   0.5298    2.8126      0.3203     285.3389    s0.5217    2.8241      0.3203  
00239   0.147    284.1194   0.5176    2.8121      0.3199     285.2504    s0.5188    2.8234      0.3203  
00240   0.183    284.0403   0.5148    2.8115      0.3198     285.1624    s0.5340    2.8217      0.3226  
00241   0.156    283.9240   0.5298    2.8095      0.3220     285.0831    s0.5397    2.8206      0.3244  
00242   0.146    283.8326   0.5355    2.8083      0.3237     284.9569    s0.5277    2.8199      0.3242  
00243   0.147    283.7242   0.5235    2.8078      0.3234     284.8688    s0.5245    2.8192      0.3249  
00244   0.147    283.6396   0.5203    2.8071      0.3241     284.8036    s0.5359    2.8180      0.3266  
00245   0.160    283.5426   0.5315    2.8056      0.3257     284.7156    s0.5357    2.8171      0.3257  
00246   0.151    283.4409   0.5313    2.8046      0.3247     284.5883    s0.5241    2.8165      0.3228  
00247   0.148    283.3358   0.5197    2.8042      0.3219     284.4809    s0.5261    2.8153      0.3212  
00248   0.147    283.2252   0.5218    2.8030      0.3203     284.4125    s0.5342    2.8142      0.3202  
00249   0.146    283.1312   0.5298    2.8016      0.3191     284.2954    s0.5284    2.8134      0.3186  
00250   0.157    283.0189   0.5240    2.8008      0.3176     284.1904    s0.5217    2.8126      0.3178  
resampling
00251   0.148    284.0300   0.5161    2.8113      0.3165     284.2925    s0.5321    2.8131      0.3245  
00252   0.148    283.8621   0.5262    2.8091      0.3236     284.2007    s0.5256    2.8125      0.3248  
00253   0.149    283.7261   0.5197    2.8080      0.3238     283.9304    s0.5129    2.8104      0.3220  
00254   0.147    283.5972   0.5071    2.8074      0.3209     283.8479    s0.5250    2.8090      0.3245  
00255   0.145    283.4364   0.5189    2.8052      0.3234     283.8531    s0.5320    2.8086      0.3287  
00256   0.147    283.3076   0.5258    2.8035      0.3277     283.6122    s0.5208    2.8068      0.3271  
00257   0.148    283.1478   0.5148    2.8025      0.3262     283.4167    s0.5251    2.8047      0.3212  
00258   0.148    283.0256   0.5190    2.8011      0.3200     283.3501    s0.5348    2.8035      0.3217  
00259   0.159    282.8584   0.5286    2.7989      0.3207     283.2984    s0.5218    2.8037      0.3229  
00260   0.152    282.7387   0.5156    2.7984      0.3219     283.1083    s0.5179    2.8020      0.3219  
00261   0.147    282.5568   0.5117    2.7968      0.3210     283.0275    s0.5294    2.8006      0.3208  
00262   0.153    282.4759   0.5230    2.7954      0.3198     282.8438    s0.5244    2.7990      0.3229  
00263   0.148    282.2813   0.5181    2.7937      0.3219     282.8084    s0.5226    2.7987      0.3239  
00264   0.150    282.2094   0.5163    2.7931      0.3229     282.6617    s0.5320    2.7968      0.3187  
00265   0.146    282.0391   0.5257    2.7909      0.3178     282.5136    s0.5236    2.7958      0.3145  
00266   0.147    281.9529   0.5173    2.7905      0.3136     282.4230    s0.5215    2.7950      0.3166  
00267   0.146    281.8162   0.5151    2.7893      0.3155     282.4254    s0.5336    2.7944      0.3173  
00268   0.147    281.7313   0.5270    2.7878      0.3163     282.2242    s0.5226    2.7930      0.3126  
00269   0.145    281.6108   0.5161    2.7872      0.3115     282.1181    s0.5200    2.7921      0.3115  
00270   0.147    281.5145   0.5136    2.7864      0.3106     282.1309    s0.5342    2.7915      0.3119  
00271   0.146    281.4325   0.5275    2.7848      0.3106     281.9428    s0.5273    2.7900      0.3094  
00272   0.148    281.3169   0.5208    2.7840      0.3085     281.8154    s0.5197    2.7892      0.3013  
00273   0.148    281.2777   0.5130    2.7841      0.2997     281.9915    s0.5392    2.7899      0.3044  
00274   0.147    281.3427   0.5328    2.7837      0.3039     282.2628    s0.5193    2.7937      0.2912  
00275   0.147    281.7528   0.5123    2.7890      0.2884     284.1808    s0.5521    2.8112      0.2989  
resampling
00276   0.150    284.2355   0.5459    2.8121      0.2984     283.8094    s0.5542    2.8076      0.2797  
00277   0.156    283.9353   0.5466    2.8093      0.2749     283.3069    s0.5132    2.8046      0.2831  
00278   0.152    283.5163   0.5072    2.8070      0.2819     281.2618    s0.5334    2.7832      0.2778  
00279   0.147    281.3960   0.5267    2.7849      0.2750     281.9218    s0.5402    2.7895      0.2726  
00280   0.146    282.1161   0.5328    2.7918      0.2687     283.5093    s0.5221    2.8063      0.2705  
00281   0.148    283.7400   0.5157    2.8089      0.2689     280.9689    s0.5390    2.7802      0.2529  
00282   0.161    281.0739   0.5318    2.7817      0.2493     281.3147    s0.5264    2.7844      0.2446  
00283   0.146    281.6148   0.5195    2.7878      0.2406     282.7836    s0.5183    2.7994      0.2555  
00284   0.147    283.0560   0.5119    2.8024      0.2535     280.3520    s0.5500    2.7735      0.2560  
00285   0.145    280.3559   0.5421    2.7739      0.2523     281.8088    s0.5386    2.7887      0.2509  
00286   0.161    282.0005   0.5310    2.7910      0.2463     282.9086    s0.5261    2.8002      0.2535  
00287   0.147    283.0252   0.5194    2.8018      0.2514     280.4033    s0.5470    2.7742      0.2437  
00288   0.155    280.2444   0.5390    2.7731      0.2403     282.8468    s0.5445    2.7989      0.2360  
00289   0.146    282.9703   0.5365    2.8006      0.2308     281.4127    s0.5015    2.7866      0.2402  
00290   0.146    281.6952   0.4950    2.7898      0.2374     280.5589    s0.5445    2.7759      0.2441  
00291   0.147    280.5874   0.5371    2.7766      0.2413     282.1720    s0.5697    2.7909      0.2371  
00292   0.148    282.0832   0.5603    2.7905      0.2313     279.5451    s0.4819    2.7691      0.2271  
00293   0.146    280.0211   0.4751    2.7742      0.2233     280.0930    s0.5159    2.7728      0.2330  
00294   0.146    280.3136   0.5088    2.7754      0.2297     279.7979    s0.5666    2.7673      0.2326  
00295   0.146    279.7312   0.5578    2.7671      0.2279     278.7661    s0.5220    2.7593      0.2267  
00296   0.147    279.0748   0.5143    2.7628      0.2219     279.0686    s0.5045    2.7631      0.2379  
00297   0.146    279.3704   0.4972    2.7665      0.2341     278.4321    s0.5493    2.7544      0.2429  
00298   0.145    278.3368   0.5410    2.7539      0.2391     278.7729    s0.5498    2.7579      0.2357  
00299   0.145    278.7122   0.5409    2.7578      0.2303     277.8950    s0.4891    2.7522      0.2274  
00300   0.147    278.4048   0.4823    2.7577      0.2231     277.4774    s0.5335    2.7457      0.2365  
resampling
00301   0.148    277.7002   0.5292    2.7482      0.2322     278.2970    s0.5657    2.7523      0.2382  
00302   0.159    278.3024   0.5597    2.7527      0.2328     278.0659    s0.4948    2.7537      0.2183  
00303   0.146    279.0061   0.4929    2.7633      0.2144     276.0583    s0.4881    2.7340      0.2185  
00304   0.146    276.5483   0.4843    2.7391      0.2137     279.3373    s0.5506    2.7635      0.2369  
00305   0.162    279.2861   0.5443    2.7633      0.2315     277.7963    s0.5263    2.7494      0.2291  
00306   0.148    278.3569   0.5238    2.7551      0.2252     276.1910    s0.4829    2.7355      0.2258  
00307   0.153    276.9902   0.4803    2.7437      0.2215     278.1732    s0.5272    2.7529      0.2445  
00308   0.158    278.4477   0.5215    2.7560      0.2390     275.1760    s0.5439    2.7221      0.2418  
00309   0.151    275.3108   0.5396    2.7238      0.2373     275.4444    s0.5263    2.7257      0.2439  
00310   0.162    275.9251   0.5231    2.7307      0.2401     274.0790    s0.5099    2.7129      0.2426  
00311   0.151    274.4421   0.5045    2.7168      0.2369     274.0136    s0.5470    2.7102      0.2541  
00312   0.152    273.9932   0.5412    2.7104      0.2483     273.6958    s0.5245    2.7082      0.2495  
00313   0.146    273.9420   0.5208    2.7109      0.2452     272.7837    s0.4944    2.7007      0.2386  
00314   0.148    273.0587   0.4892    2.7038      0.2334     272.6709    s0.5566    2.6963      0.2582  
00315   0.155    272.2738   0.5502    2.6927      0.2522     272.6521    s0.5187    2.6980      0.2567  
00316   0.148    272.7307   0.5148    2.6990      0.2527     271.2714    s0.5182    2.6842      0.2607  
00317   0.147    271.4605   0.5121    2.6864      0.2551     270.4214    s0.5772    2.6726      0.2743  
00318   0.157    270.2097   0.5716    2.6708      0.2692     269.7590    s0.5091    2.6695      0.2610  
00319   0.162    269.6391   0.5037    2.6686      0.2570     270.2183    s0.5627    2.6714      0.2691  
00320   0.155    269.8544   0.5555    2.6681      0.2629     271.5446    s0.5091    2.6875      0.2491  
00321   0.147    271.9364   0.5063    2.6916      0.2467     268.2118    s0.5368    2.6528      0.2512  
00322   0.147    268.1270   0.5302    2.6523      0.2458     268.5956    s0.5439    2.6562      0.2601  
00323   0.146    268.1757   0.5370    2.6524      0.2554     270.7352    s0.5317    2.6783      0.2466  
00324   0.148    270.7517   0.5285    2.6786      0.2454     269.5451    s0.5632    2.6650      0.2280  
00325   0.152    269.6719   0.5576    2.6666      0.2244     268.0094    s0.5127    2.6524      0.2008  
resampling
00326   0.156    266.7815   0.5089    2.6404      0.1985     271.4792    s0.4500    2.6905      0.1751  
00327   0.149    269.6883   0.4458    2.6729      0.1731     271.4110    s0.6411    2.6795      0.2578  
00328   0.150    270.6119   0.6379    2.6716      0.2574     267.4224    s0.5863    2.6428      0.2104  
00329   0.151    265.7552   0.5825    2.6263      0.2081     273.1727    s0.3943    2.7105      0.1527  
00330   0.175    271.2476   0.3890    2.6915      0.1499     266.9067    s0.6183    2.6361      0.2087  
00331   0.148    265.8702   0.6135    2.6260      0.2066     269.8921    s0.6241    2.6656      0.2072  
00332   0.150    268.8709   0.6202    2.6556      0.2066     265.0485    s0.4715    2.6252      0.1660  
00333   0.152    263.6539   0.4670    2.6115      0.1644     268.4768    s0.4672    2.6595      0.1868  
00334   0.150    267.0815   0.4621    2.6459      0.1839     268.4276    s0.5801    2.6534      0.1900  
00335   0.148    266.6754   0.5757    2.6361      0.1871     267.9749    s0.5470    2.6504      0.1977  
00336   0.158    266.6616   0.5433    2.6375      0.1980     267.0230    s0.5648    2.6399      0.2060  
00337   0.147    265.2626   0.5619    2.6225      0.2052     263.8277    s0.5309    2.6097      0.2082  
00338   0.158    262.2167   0.5267    2.5938      0.2067     266.2149    s0.4816    2.6361      0.1949  
00339   0.146    265.0161   0.4759    2.6244      0.1931     264.4543    s0.5752    2.6140      0.1810  
00340   0.147    262.5801   0.5704    2.5955      0.1790     265.6966    s0.5539    2.6273      0.1964  
00341   0.146    264.2806   0.5502    2.6133      0.1949     263.5963    s0.5096    2.6086      0.1907  
00342   0.146    262.2046   0.5056    2.5949      0.1899     263.8215    s0.5250    2.6098      0.2133  
00343   0.148    262.4483   0.5202    2.5964      0.2119     264.0765    s0.6071    2.6082      0.2181  
00344   0.160    262.2846   0.6028    2.5905      0.2163     263.1613    s0.5584    2.6017      0.1973  
00345   0.147    261.1122   0.5550    2.5814      0.1960     262.9737    s0.4826    2.6038      0.1818  
00346   0.149    261.1452   0.4785    2.5857      0.1809     261.4094    s0.5330    2.5855      0.1981  
00347   0.150    259.6516   0.5284    2.5681      0.1966     262.2264    s0.5744    2.5914      0.2186  
c:\Users\shark\OneDrive\×ÀÃæ\OT-Flow-master\trainToyOTflow_2d.py
start time: 2022_03_25_13_41_35
Namespace(alph=[1.0, 100.0, 5.0], alphaa=10, batch_size=4096, data='moons', drop_freq=200, gpu=0, logrho0_freq=1000, lr=0.05, lr_drop=2.0, m=32, nTh=2, niters=10000, nt=8, nt_val=8, optim='adam', prec='double', sample_freq=25, save='experiments/cnf/toy', val_batch_size=4096, val_freq=1, viz_freq=100, weight_decay=0.0)
Phi(
  (c): Linear(in_features=3, out_features=1, bias=True)
  (w): Linear(in_features=32, out_features=1, bias=False)
  (N): ResNN(
    (layers): ModuleList(
      (0): Linear(in_features=3, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
)
-------------------------
DIMENSION=2  m=32  nTh=2   alpha=[1.0, 100.0, 5.0]
nt=8   nt_val=8
Number of trainable parameters: 1229
-------------------------
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.05
    maximize: False
    weight_decay: 0.0
)
data=moons batch_size=4096 gpu=0
maxIters=10000 val_freq=1 viz_freq=100
saveLocation = experiments/cnf/toy
-------------------------

iter    time    loss       L (L_2)    C (loss)   R (HJB)   V(velocity)     valLoss    valL       valC       valR      valV      
00001   1.926    1238.0658   5.0104    11.5092      62.0971     1253.2954    s5.2355    11.5408      73.0387  
00002   0.155    1253.5878   5.2584    11.5396      73.3340     778.4480    s3.2119    7.3848      23.9044  
00003   0.151    778.5064   3.2240    7.3841      23.9723     534.5110    s2.0101    5.1646      8.0049  
00004   0.151    534.5833   2.0164    5.1648      8.0192     450.6508    s1.5236    4.3877      4.2640  
00005   0.155    450.8157   1.5280    4.3890      4.2713     398.9888    s1.1420    3.9100      2.2795  
00006   0.151    399.2806   1.1448    3.9127      2.2839     364.6415    s0.7902    3.5955      1.1358  
00007   0.155    365.0990   0.7919    3.6000      1.1383     347.1979    s0.5517    3.4388      0.5614  
00008   0.157    347.7830   0.5529    3.4446      0.5628     339.4795    s0.4311    3.3704      0.2882  
00009   0.163    340.1149   0.4320    3.3767      0.2889     338.1204    s0.3670    3.3612      0.1672  
00010   0.148    338.7778   0.3677    3.3677      0.1676     338.0166    s0.2822    3.3648      0.1222  
00011   0.153    338.7011   0.2827    3.3717      0.1226     338.7995    s0.2750    3.3731      0.1191  
00012   0.148    339.4404   0.2756    3.3794      0.1194     339.5124    s0.3470    3.3763      0.1510  
00013   0.160    340.0582   0.3478    3.3817      0.1512     339.9462    s0.3748    3.3786      0.2165  
00014   0.156    340.4709   0.3757    3.3838      0.2168     341.0968    s0.3871    3.3884      0.3199  
00015   0.151    341.6437   0.3879    3.3938      0.3202     341.7810    s0.4652    3.3900      0.4568  
00016   0.148    342.2867   0.4662    3.3950      0.4571     343.1433    s0.5433    3.3983      0.5961  
00017   0.151    343.6134   0.5445    3.4029      0.5963     343.0075    s0.5222    3.3970      0.6937  
00018   0.150    343.5246   0.5233    3.4021      0.6937     342.3457    s0.5102    3.3906      0.7303  
00019   0.148    342.8952   0.5110    3.3961      0.7303     341.1118    s0.5202    3.3781      0.7053  
00020   0.148    341.6464   0.5211    3.3834      0.7052     339.3977    s0.4589    3.3646      0.6437  
00021   0.149    339.9675   0.4598    3.3702      0.6437     338.3234    s0.3981    3.3576      0.5775  
00022   0.148    338.9516   0.3988    3.3638      0.5775     337.5354    s0.3951    3.3504      0.5213  
00023   0.154    338.1721   0.3957    3.3567      0.5212     336.7216    s0.3566    3.3446      0.4834  
00024   0.148    337.3886   0.3573    3.3512      0.4834     336.2888    s0.3254    3.3420      0.4651  
00025   0.149    337.0082   0.3260    3.3491      0.4651     335.7081    s0.3510    3.3349      0.4620  
resampling
00026   0.149    336.0430   0.3500    3.3383      0.4595     334.7151    s0.3360    3.3256      0.4798  
00027   0.160    335.1263   0.3349    3.3297      0.4772     333.9987    s0.3470    3.3175      0.5123  
00028   0.154    334.4624   0.3460    3.3222      0.5095     333.4720    s0.3955    3.3094      0.5519  
00029   0.153    333.9305   0.3942    3.3141      0.5489     332.8473    s0.3832    3.3032      0.6069  
00030   0.158    333.4498   0.3819    3.3094      0.6037     332.5194    s0.4477    3.2963      0.6518  
00031   0.149    333.0848   0.4463    3.3020      0.6483     332.2006    s0.4473    3.2927      0.6923  
00032   0.159    332.8751   0.4459    3.2996      0.6886     332.1395    s0.4685    3.2909      0.7080  
00033   0.154    332.8501   0.4670    3.2981      0.7043     332.1068    s0.4890    3.2897      0.6954  
00034   0.159    332.8186   0.4876    3.2969      0.6917     331.9285    s0.4609    3.2896      0.6633  
00035   0.149    332.7252   0.4596    3.2977      0.6599     331.9805    s0.4932    3.2891      0.6072  
00036   0.150    332.6855   0.4918    3.2962      0.6042     331.9084    s0.4184    3.2926      0.5574  
00037   0.148    332.7620   0.4173    3.3012      0.5547     332.1453    s0.4955    3.2917      0.4963  
00038   0.154    332.7703   0.4943    3.2981      0.4939     330.9438    s0.3861    3.2856      0.4576  
00039   0.151    331.7632   0.3851    3.2938      0.4556     329.6585    s0.4160    3.2716      0.4189  
00040   0.149    330.3437   0.4149    3.2785      0.4172     329.4899    s0.4345    3.2693      0.3904  
00041   0.167    330.1134   0.4335    3.2756      0.3889     329.2313    s0.3846    3.2694      0.3693  
00042   0.150    329.9614   0.3837    3.2768      0.3680     328.2888    s0.4248    3.2582      0.3496  
00043   0.154    328.9167   0.4237    3.2645      0.3485     327.7151    s0.4402    3.2519      0.3235  
00044   0.158    328.3089   0.4393    3.2579      0.3226     327.1716    s0.4104    3.2483      0.2933  
00045   0.153    327.8274   0.4095    3.2549      0.2925     325.8949    s0.4328    3.2347      0.2622  
00046   0.149    326.4740   0.4319    3.2405      0.2617     325.1310    s0.4397    3.2270      0.2298  
00047   0.148    325.6749   0.4389    3.2325      0.2294     324.2669    s0.4089    3.2202      0.1996  
00048   0.148    324.8637   0.4083    3.2262      0.1994     323.0453    s0.4332    3.2070      0.1745  
00049   0.148    323.5772   0.4327    3.2124      0.1744     322.0598    s0.4465    3.1967      0.1558  
00050   0.147    322.5823   0.4459    3.2020      0.1559     320.7688    s0.4236    3.1851      0.1410  
resampling
00051   0.149    321.4655   0.4230    3.1921      0.1410     319.2008    s0.4552    3.1680      0.1232  
00052   0.148    319.8848   0.4547    3.1749      0.1233     317.7728    s0.4590    3.1537      0.1059  
00053   0.148    318.4524   0.4587    3.1605      0.1061     316.4066    s0.4148    3.1424      0.0904  
00054   0.155    317.1069   0.4146    3.1494      0.0906     315.0006    s0.4368    3.1274      0.0722  
00055   0.147    315.6594   0.4369    3.1340      0.0724     313.5765    s0.4110    3.1146      0.0652  
00056   0.146    314.2465   0.4112    3.1213      0.0654     312.3403    s0.4269    3.1014      0.0642  
00057   0.152    313.0055   0.4272    3.1080      0.0645     311.5330    s0.4635    3.0915      0.0647  
00058   0.147    312.1626   0.4640    3.0978      0.0651     311.0284    s0.4132    3.0889      0.0678  
00059   0.147    311.6465   0.4137    3.0951      0.0681     311.6598    s0.4768    3.0921      0.0701  
00060   0.148    312.1303   0.4775    3.0967      0.0706     314.6901    s0.4471    3.1236      0.0993  
00061   0.147    315.3230   0.4477    3.1298      0.0997     321.0301    s0.6166    3.1784      0.1030  
00062   0.160    321.2263   0.6171    3.1804      0.1032     322.5921    s0.5028    3.1995      0.1295  
00063   0.147    323.2506   0.5034    3.2060      0.1296     311.2082    s0.4408    3.0891      0.0967  
00064   0.146    311.5034   0.4417    3.0920      0.0971     320.8369    s0.5939    3.1775      0.1200  
00065   0.148    320.9346   0.5942    3.1784      0.1200     317.8735    s0.5081    3.1521      0.1266  
00066   0.147    318.4100   0.5089    3.1574      0.1266     314.3719    s0.4642    3.1194      0.1119  
00067   0.147    314.8408   0.4650    3.1240      0.1121     317.8014    s0.5296    3.1504      0.1092  
00068   0.149    317.9861   0.5300    3.1523      0.1093     311.5712    s0.4928    3.0901      0.0983  
00069   0.150    311.8975   0.4935    3.0933      0.0985     314.3753    s0.4961    3.1179      0.1060  
00070   0.146    314.9647   0.4965    3.1238      0.1062     311.4604    s0.4837    3.0894      0.0976  
00071   0.146    312.0349   0.4841    3.0952      0.0979     312.1448    s0.5017    3.0955      0.0914  
00072   0.148    312.5652   0.5021    3.0996      0.0916     311.9443    s0.4632    3.0955      0.0827  
00073   0.147    312.3928   0.4635    3.0999      0.0828     310.4303    s0.4197    3.0826      0.0744  
00074   0.147    311.0502   0.4200    3.0888      0.0745     311.6064    s0.4563    3.0924      0.0802  
00075   0.147    312.2980   0.4565    3.0994      0.0803     310.0094    s0.4963    3.0744      0.0868  
resampling
00076   0.155    310.4770   0.4988    3.0790      0.0861     310.7739    s0.4961    3.0820      0.0887  
00077   0.149    311.2750   0.4987    3.0869      0.0882     309.9254    s0.4185    3.0776      0.0766  
00078   0.147    310.3525   0.4208    3.0817      0.0761     310.2161    s0.3897    3.0819      0.0739  
00079   0.148    310.6597   0.3920    3.0863      0.0731     309.4675    s0.4424    3.0717      0.0895  
00080   0.147    309.9795   0.4448    3.0767      0.0885     309.3313    s0.5132    3.0666      0.1054  
00081   0.148    309.8033   0.5158    3.0712      0.1046     309.3732    s0.5054    3.0674      0.1076  
00082   0.148    309.7691   0.5080    3.0712      0.1068     308.3499    s0.4306    3.0610      0.0994  
00083   0.147    308.7742   0.4329    3.0651      0.0985     308.9824    s0.4105    3.0683      0.1025  
00084   0.148    309.4687   0.4128    3.0730      0.1013     307.7139    s0.4630    3.0528      0.1192  
00085   0.147    308.1242   0.4654    3.0568      0.1181     308.5528    s0.5188    3.0582      0.1345  
00086   0.150    308.8529   0.5213    3.0611      0.1335     307.5817    s0.4867    3.0501      0.1376  
00087   0.148    307.9102   0.4890    3.0533      0.1364     307.8340    s0.4284    3.0556      0.1362  
00088   0.158    308.2565   0.4306    3.0597      0.1346     307.3736    s0.4365    3.0505      0.1450  
00089   0.148    307.7440   0.4387    3.0541      0.1435     307.5554    s0.5035    3.0487      0.1653  
00090   0.146    307.8286   0.5057    3.0514      0.1639     307.2248    s0.5152    3.0447      0.1783  
00091   0.148    307.5548   0.5175    3.0479      0.1766     307.0343    s0.4676    3.0452      0.1777  
00092   0.147    307.4556   0.4699    3.0493      0.1756     306.8438    s0.4562    3.0439      0.1779  
00093   0.148    307.2053   0.4584    3.0474      0.1760     306.7577    s0.5009    3.0406      0.1921  
00094   0.148    307.0611   0.5031    3.0436      0.1904     306.4122    s0.5142    3.0364      0.2018  
00095   0.147    306.7981   0.5164    3.0402      0.1998     306.1536    s0.4792    3.0356      0.1972  
00096   0.148    306.5812   0.4814    3.0398      0.1951     306.0328    s0.4615    3.0353      0.1934  
00097   0.152    306.4004   0.4636    3.0389      0.1915     305.7400    s0.4815    3.0313      0.2018  
00098   0.147    306.1254   0.4836    3.0351      0.2000     305.5961    s0.4949    3.0291      0.2090  
00099   0.148    306.0557   0.4971    3.0336      0.2070     305.2724    s0.4793    3.0267      0.2075  
00100   0.148    305.7180   0.4815    3.0311      0.2055     305.2718    s0.4704    3.0271      0.2085  
resampling
00101   0.149    305.6623   0.4735    3.0309      0.2087     304.9087    s0.4721    3.0233      0.2148  
00102   0.150    305.3141   0.4752    3.0272      0.2149     304.7756    s0.4870    3.0212      0.2223  
00103   0.151    305.2100   0.4900    3.0254      0.2223     304.4900    s0.4913    3.0181      0.2282  
00104   0.152    304.9092   0.4945    3.0221      0.2284     304.3607    s0.4728    3.0177      0.2308  
00105   0.147    304.7355   0.4761    3.0212      0.2311     304.0779    s0.4677    3.0151      0.2338  
00106   0.156    304.4557   0.4707    3.0187      0.2340     303.7845    s0.4995    3.0104      0.2475  
00107   0.147    304.1780   0.5028    3.0142      0.2479     303.5129    s0.4909    3.0081      0.2524  
00108   0.148    303.8654   0.4942    3.0114      0.2530     303.4158    s0.4637    3.0085      0.2500  
00109   0.149    303.7360   0.4668    3.0115      0.2503     302.9755    s0.5107    3.0015      0.2714  
00110   0.158    303.3120   0.5142    3.0047      0.2721     302.5616    s0.5001    2.9979      0.2758  
00111   0.159    302.8759   0.5036    3.0008      0.2764     302.3232    s0.4808    2.9964      0.2777  
00112   0.147    302.5811   0.4842    2.9988      0.2785     301.8230    s0.5145    2.9895      0.2969  
00113   0.148    302.1068   0.5182    2.9922      0.2979     301.3421    s0.5028    2.9853      0.2997  
00114   0.158    301.6174   0.5063    2.9879      0.3005     301.1309    s0.5164    2.9824      0.3107  
00115   0.148    301.3877   0.5204    2.9847      0.3123     303.0901    s0.4958    3.0031      0.3034  
00116   0.156    303.3542   0.4990    3.0056      0.3037     320.2702    s0.8551    3.1560      0.3981  
00117   0.147    320.5481   0.8605    3.1585      0.4007     391.4636    s1.0193    3.8612      0.2422  
00118   0.158    391.3230   1.0203    3.8598      0.2414     337.2313    s0.9538    3.3216      0.2992  
00119   0.148    337.6476   0.9567    3.3256      0.2992     344.4081    s0.9356    3.3946      0.2700  
00120   0.148    344.6165   0.9395    3.3965      0.2708     325.8468    s0.4129    3.2363      0.1537  
00121   0.155    325.8285   0.4151    3.2360      0.1542     331.4327    s0.3518    3.2958      0.0946  
00122   0.158    331.5781   0.3529    3.2972      0.0942     315.6330    s0.4059    3.1351      0.0964  
00123   0.147    316.1333   0.4071    3.1400      0.0957     319.1366    s0.7160    3.1540      0.1522  
00124   0.148    319.8016   0.7183    3.1606      0.1520     322.7023    s0.8289    3.1840      0.1615  
00125   0.147    323.3625   0.8314    3.1904      0.1616     316.9438    s0.6562    3.1355      0.1090  
resampling
00126   0.163    317.7860   0.6578    3.1439      0.1098     316.7866    s0.4943    3.1425      0.0606  
00127   0.147    317.7471   0.4958    3.1521      0.0610     317.9607    s0.4011    3.1591      0.0405  
00128   0.148    318.9958   0.4024    3.1694      0.0407     316.8462    s0.3510    3.1505      0.0368  
00129   0.146    317.9612   0.3523    3.1616      0.0368     314.5496    s0.3378    3.1282      0.0426  
00130   0.149    315.7255   0.3390    3.1399      0.0426     311.8346    s0.3668    3.0995      0.0554  
00131   0.147    313.0084   0.3678    3.1111      0.0555     309.9167    s0.4392    3.0765      0.0743  
00132   0.148    311.0405   0.4400    3.0877      0.0746     309.9178    s0.5310    3.0717      0.0954  
00133   0.147    310.9938   0.5318    3.0824      0.0961     310.6584    s0.5949    3.0757      0.1114  
00134   0.146    311.7334   0.5956    3.0864      0.1123     309.8129    s0.5928    3.0673      0.1166  
00135   0.148    310.9538   0.5936    3.0787      0.1175     307.5187    s0.5307    3.0475      0.1126  
00136   0.147    308.7892   0.5315    3.0602      0.1133     306.2436    s0.4495    3.0389      0.1060  
00137   0.148    307.6885   0.4504    3.0533      0.1063     306.8700    s0.3909    3.0481      0.1024  
00138   0.148    308.4971   0.3920    3.0643      0.1022     307.2270    s0.3763    3.0524      0.1070  
00139   0.146    308.9789   0.3775    3.0699      0.1066     306.0128    s0.4064    3.0386      0.1232  
00140   0.147    307.7778   0.4074    3.0562      0.1229     305.0383    s0.4663    3.0256      0.1478  
00141   0.152    306.7338   0.4670    3.0425      0.1477     305.0708    s0.5249    3.0227      0.1726  
00142   0.146    306.7085   0.5254    3.0391      0.1727     305.1533    s0.5526    3.0220      0.1895  
00143   0.148    306.8163   0.5530    3.0386      0.1895     305.1819    s0.5412    3.0228      0.1949  
00144   0.152    306.9534   0.5417    3.0405      0.1946     304.5324    s0.5101    3.0179      0.1940  
00145   0.147    306.4199   0.5108    3.0367      0.1933     303.6166    s0.4944    3.0095      0.1974  
00146   0.147    305.5263   0.4950    3.0285      0.1967     304.0675    s0.4984    3.0137      0.2032  
00147   0.147    305.9247   0.4989    3.0323      0.2026     304.0078    s0.4909    3.0135      0.2020  
00148   0.147    305.8397   0.4914    3.0318      0.2015     303.2413    s0.4716    3.0069      0.1958  
00149   0.147    305.0767   0.4723    3.0252      0.1953     303.1369    s0.4708    3.0059      0.1940  
00150   0.147    304.9270   0.4715    3.0238      0.1936     302.8578    s0.4930    3.0019      0.1990  
resampling
00151   0.148    303.1925   0.4958    3.0052      0.1974     302.4117    s0.5051    2.9968      0.2027  
00152   0.146    302.8327   0.5079    3.0009      0.2011     301.8528    s0.4972    2.9916      0.2020  
00153   0.148    302.2735   0.5000    2.9957      0.2004     301.4374    s0.4797    2.9884      0.1971  
00154   0.147    301.7740   0.4825    2.9917      0.1957     301.1477    s0.4739    2.9858      0.1941  
00155   0.148    301.4102   0.4765    2.9883      0.1929     300.5530    s0.4889    2.9791      0.1990  
00156   0.147    300.8471   0.4913    2.9819      0.1979     300.0043    s0.5096    2.9725      0.2071  
00157   0.147    300.3973   0.5121    2.9763      0.2060     299.4893    s0.5113    2.9672      0.2100  
00158   0.147    299.9332   0.5137    2.9716      0.2088     298.8565    s0.4874    2.9621      0.2068  
00159   0.152    299.2564   0.4898    2.9660      0.2057     298.5209    s0.4582    2.9603      0.2034  
00160   0.145    298.8362   0.4607    2.9633      0.2024     298.1175    s0.4503    2.9566      0.2069  
00161   0.150    298.4033   0.4528    2.9593      0.2059     297.2358    s0.4708    2.9466      0.2195  
00162   0.147    297.5556   0.4733    2.9497      0.2185     296.5402    s0.5026    2.9379      0.2361  
00163   0.147    296.8547   0.5052    2.9409      0.2351     296.0496    s0.5181    2.9321      0.2498  
00164   0.147    296.2671   0.5206    2.9342      0.2488     295.2348    s0.5061    2.9245      0.2588  
00165   0.147    295.3513   0.5086    2.9255      0.2579     294.5705    s0.4899    2.9185      0.2697  
00166   0.147    294.6425   0.4927    2.9191      0.2689     293.9301    s0.4924    2.9118      0.2871  
00167   0.147    293.9766   0.4954    2.9121      0.2864     293.1675    s0.5120    2.9030      0.3091  
00168   0.146    293.2017   0.5151    2.9032      0.3084     292.3383    s0.5255    2.8938      0.3296  
00169   0.147    292.4414   0.5287    2.8947      0.3289     291.5411    s0.5216    2.8859      0.3421  
00170   0.147    291.7860   0.5254    2.8882      0.3415     290.9448    s0.5193    2.8799      0.3535  
00171   0.147    291.2676   0.5236    2.8830      0.3531     290.4884    s0.5303    2.8746      0.3746  
00172   0.148    290.8281   0.5347    2.8778      0.3744     290.2329    s0.5247    2.8722      0.3886  
00173   0.147    290.5568   0.5294    2.8752      0.3887     290.0324    s0.5111    2.8709      0.3917  
00174   0.146    290.2762   0.5160    2.8730      0.3921     290.0395    s0.5330    2.8697      0.3999  
00175   0.148    290.2126   0.5378    2.8712      0.4005     289.9748    s0.5638    2.8674      0.4150  
resampling
00176   0.149    289.4457   0.5662    2.8620      0.4175     290.1531    s0.5433    2.8701      0.4217  
00177   0.146    289.3702   0.5457    2.8622      0.4256     290.1317    s0.4958    2.8724      0.4085  
00178   0.147    289.2435   0.4986    2.8634      0.4115     289.9024    s0.5153    2.8691      0.4119  
00179   0.152    289.0955   0.5182    2.8609      0.4147     289.8361    s0.5511    2.8666      0.4192  
00180   0.146    289.0686   0.5538    2.8588      0.4224     289.5794    s0.5282    2.8653      0.4035  
00181   0.151    288.7548   0.5308    2.8569      0.4058     289.5043    s0.5148    2.8653      0.3965  
00182   0.147    288.5686   0.5168    2.8559      0.3988     289.0414    s0.5410    2.8594      0.3931  
00183   0.146    288.1540   0.5430    2.8504      0.3954     288.6403    s0.5338    2.8559      0.3775  
00184   0.148    287.8716   0.5361    2.8481      0.3788     288.5448    s0.5041    2.8565      0.3698  
00185   0.146    287.7043   0.5061    2.8480      0.3707     288.3628    s0.5136    2.8543      0.3658  
00186   0.150    287.3900   0.5154    2.8445      0.3672     287.8458    s0.5278    2.8485      0.3612  
00187   0.147    287.0275   0.5292    2.8402      0.3616     287.6551    s0.5274    2.8467      0.3512  
00188   0.146    286.9564   0.5286    2.8396      0.3506     287.6294    s0.5377    2.8459      0.3524  
00189   0.147    286.7743   0.5385    2.8373      0.3522     287.4867    s0.5178    2.8455      0.3500  
00190   0.146    286.6534   0.5187    2.8371      0.3497     287.4257    s0.5067    2.8456      0.3353  
00191   0.147    286.5818   0.5074    2.8371      0.3340     287.1698    s0.5442    2.8411      0.3395  
00192   0.146    286.5962   0.5452    2.8353      0.3384     287.2688    s0.5073    2.8441      0.3203  
00193   0.147    286.4306   0.5077    2.8357      0.3187     287.1685    s0.5132    2.8428      0.3265  
00194   0.147    286.5064   0.5140    2.8361      0.3253     287.4537    s0.5199    2.8454      0.3118  
00195   0.146    286.5152   0.5199    2.8361      0.3104     287.2457    s0.5185    2.8434      0.3167  
00196   0.149    286.7765   0.5195    2.8386      0.3148     287.6611    s0.5169    2.8477      0.3029  
00197   0.146    286.7064   0.5170    2.8382      0.3009     287.8224    s0.5241    2.8488      0.3197  
00198   0.146    287.3572   0.5252    2.8441      0.3179     287.9572    s0.5014    2.8515      0.2978  
00199   0.147    286.8467   0.5012    2.8404      0.2956     287.5953    s0.5448    2.8456      0.3131  
00200   0.146    287.2254   0.5458    2.8419      0.3108     287.0590    s0.5191    2.8417      0.2937  
resampling
00201   0.187    286.6625   0.5214    2.8376      0.2951     286.0696    s0.5177    2.8318      0.2994  
00202   0.148    285.7683   0.5202    2.8287      0.3007     286.4761    s0.5253    2.8354      0.3070  
00203   0.148    286.2464   0.5278    2.8330      0.3077     286.0687    s0.5183    2.8317      0.3036  
00204   0.162    285.7210   0.5208    2.8281      0.3048     286.6685    s0.5137    2.8380      0.2959  
00205   0.148    286.1027   0.5160    2.8323      0.2974     286.0099    s0.5214    2.8310      0.2981  
00206   0.148    285.5984   0.5239    2.8268      0.2993     286.1086    s0.5344    2.8313      0.3040  
00207   0.148    285.8625   0.5370    2.8287      0.3046     285.8493    s0.5297    2.8290      0.3015  
00208   0.148    285.5150   0.5323    2.8255      0.3027     286.3144    s0.5200    2.8342      0.2941  
00209   0.148    285.7773   0.5225    2.8287      0.2958     285.8320    s0.5238    2.8292      0.2950  
00210   0.146    285.4242   0.5264    2.8250      0.2963     285.8639    s0.5345    2.8289      0.2973  
00211   0.147    285.5954   0.5371    2.8261      0.2980     285.6789    s0.5299    2.8274      0.2921  
00212   0.147    285.3120   0.5324    2.8236      0.2929     286.0450    s0.5183    2.8317      0.2851  
00213   0.147    285.4811   0.5207    2.8259      0.2863     285.7067    s0.5153    2.8284      0.2857  
00214   0.148    285.2230   0.5177    2.8235      0.2867     285.7214    s0.5215    2.8283      0.2868  
00215   0.146    285.3459   0.5240    2.8244      0.2874     285.5375    s0.5207    2.8265      0.2805  
00216   0.146    285.1172   0.5231    2.8222      0.2813     285.7254    s0.5177    2.8286      0.2734  
00217   0.147    285.2398   0.5201    2.8237      0.2743     285.4259    s0.5190    2.8256      0.2740  
00218   0.147    285.0515   0.5214    2.8217      0.2746     285.4615    s0.5223    2.8257      0.2755  
00219   0.147    285.1256   0.5248    2.8223      0.2760     285.4150    s0.5175    2.8256      0.2709  
00220   0.148    284.9581   0.5198    2.8209      0.2717     285.5540    s0.5123    2.8273      0.2665  
00221   0.147    285.0146   0.5145    2.8217      0.2673     285.3195    s0.5159    2.8247      0.2682  
00222   0.146    284.8918   0.5182    2.8203      0.2686     285.3054    s0.5207    2.8243      0.2682  
00223   0.149    284.9076   0.5231    2.8202      0.2685     285.3138    s0.5171    2.8246      0.2633  
00224   0.147    284.8128   0.5194    2.8195      0.2639     285.3136    s0.5132    2.8249      0.2609  
00225   0.146    284.7974   0.5155    2.8196      0.2615     285.1675    s0.5176    2.8232      0.2629  
resampling
00226   0.152    284.8098   0.5186    2.8195      0.2647     285.1159    s0.5288    2.8221      0.2583  
00227   0.147    284.6737   0.5294    2.8177      0.2600     285.2762    s0.5275    2.8239      0.2511  
00228   0.148    284.6829   0.5277    2.8179      0.2528     285.0833    s0.5152    2.8226      0.2498  
00229   0.147    284.5412   0.5158    2.8171      0.2515     285.0858    s0.5111    2.8228      0.2503  
00230   0.148    284.5354   0.5118    2.8172      0.2522     285.1836    s0.5106    2.8238      0.2459  
00231   0.147    284.4691   0.5110    2.8167      0.2480     285.1939    s0.5080    2.8241      0.2408  
00232   0.149    284.4297   0.5082    2.8165      0.2428     285.0215    s0.5107    2.8223      0.2403  
00233   0.148    284.3833   0.5111    2.8159      0.2419     284.9706    s0.5175    2.8214      0.2407  
00234   0.149    284.3250   0.5180    2.8149      0.2423     285.0221    s0.5138    2.8221      0.2391  
00235   0.148    284.2685   0.5142    2.8146      0.2409     284.9276    s0.5054    2.8216      0.2387  
00236   0.147    284.2087   0.5060    2.8144      0.2405     284.7751    s0.5136    2.8197      0.2404  
00237   0.149    284.1475   0.5143    2.8133      0.2420     284.7593    s0.5239    2.8190      0.2395  
00238   0.155    284.0890   0.5244    2.8123      0.2410     284.7675    s0.5173    2.8194      0.2370  
00239   0.145    284.0403   0.5177    2.8121      0.2385     284.6479    s0.5103    2.8186      0.2370  
00240   0.147    283.9723   0.5109    2.8118      0.2385     284.5811    s0.5167    2.8176      0.2369  
00241   0.147    283.9219   0.5173    2.8110      0.2384     284.5594    s0.5174    2.8174      0.2325  
00242   0.148    283.8424   0.5179    2.8102      0.2340     284.5000    s0.5113    2.8172      0.2282  
00243   0.147    283.7868   0.5117    2.8100      0.2295     284.3854    s0.5135    2.8159      0.2279  
00244   0.150    283.7096   0.5140    2.8091      0.2292     284.3663    s0.5138    2.8157      0.2266  
00245   0.147    283.6396   0.5142    2.8084      0.2280     284.3615    s0.5057    2.8161      0.2232  
00246   0.146    283.5674   0.5061    2.8081      0.2246     284.2459    s0.5079    2.8148      0.2227  
00247   0.147    283.4795   0.5083    2.8071      0.2239     284.1570    s0.5166    2.8135      0.2238  
00248   0.148    283.4074   0.5169    2.8060      0.2249     284.1157    s0.5123    2.8133      0.2223  
00249   0.147    283.3101   0.5125    2.8052      0.2234     284.0535    s0.5076    2.8129      0.2210  
00250   0.147    283.2285   0.5078    2.8047      0.2221     283.9088    s0.5151    2.8111      0.2214  
resampling
00251   0.160    284.7479   0.5174    2.8194      0.2234     284.0541    s0.5298    2.8118      0.2239  
00252   0.149    284.7056   0.5315    2.8182      0.2251     283.7128    s0.5111    2.8094      0.2132  
00253   0.149    284.4108   0.5131    2.8163      0.2148     283.6678    s0.5040    2.8094      0.2043  
00254   0.148    284.4297   0.5062    2.8169      0.2064     283.5451    s0.5201    2.8074      0.2076  
00255   0.152    284.2430   0.5219    2.8142      0.2092     283.4813    s0.5113    2.8072      0.2047  
00256   0.148    284.1467   0.5128    2.8138      0.2057     283.3324    s0.5001    2.8064      0.1969  
00257   0.148    284.0192   0.5019    2.8131      0.1983     283.1585    s0.5139    2.8039      0.2006  
00258   0.148    283.8252   0.5157    2.8104      0.2021     283.0666    s0.5133    2.8029      0.2057  
00259   0.147    283.7477   0.5146    2.8097      0.2065     282.8024    s0.5044    2.8008      0.2014  
00260   0.146    283.5128   0.5059    2.8078      0.2025     282.7773    s0.5127    2.8002      0.1982  
00261   0.147    283.4080   0.5147    2.8063      0.2001     282.5844    s0.5110    2.7983      0.1998  
00262   0.157    283.1814   0.5127    2.8042      0.2012     282.4015    s0.5063    2.7967      0.1982  
00263   0.147    283.0197   0.5079    2.8028      0.1997     282.2852    s0.5134    2.7952      0.1955  
00264   0.148    282.8712   0.5154    2.8010      0.1977     282.0418    s0.5124    2.7928      0.1974  
00265   0.147    282.5933   0.5142    2.7982      0.1992     281.8932    s0.5126    2.7913      0.1999  
00266   0.152    282.4200   0.5139    2.7965      0.2011     281.6429    s0.5183    2.7885      0.1979  
00267   0.146    282.1495   0.5198    2.7935      0.1995     281.3804    s0.5132    2.7862      0.1965  
00268   0.148    281.9108   0.5146    2.7914      0.1980     281.1652    s0.5129    2.7840      0.1974  
00269   0.159    281.6665   0.5140    2.7890      0.1986     280.9267    s0.5121    2.7817      0.1929  
00270   0.148    281.3609   0.5134    2.7860      0.1944     280.6659    s0.5091    2.7793      0.1894  
00271   0.147    281.0894   0.5105    2.7835      0.1912     280.3403    s0.5162    2.7757      0.1938  
00272   0.146    280.7800   0.5176    2.7800      0.1955     280.0403    s0.5092    2.7730      0.1915  
00273   0.148    280.4252   0.5106    2.7768      0.1934     279.7922    s0.5115    2.7704      0.1904  
00274   0.147    280.0870   0.5130    2.7733      0.1926     279.3703    s0.5173    2.7659      0.1952  
00275   0.148    279.6965   0.5188    2.7691      0.1971     279.0132    s0.5131    2.7625      0.1952  
resampling
00276   0.148    278.9415   0.5172    2.7616      0.1944     278.9050    s0.4847    2.7629      0.1878  
00277   0.146    278.6666   0.4886    2.7604      0.1871     278.6737    s0.5516    2.7571      0.2074  
00278   0.147    278.5816   0.5557    2.7560      0.2071     278.8575    s0.4521    2.7642      0.1806  
00279   0.149    278.5014   0.4558    2.7604      0.1799     279.0865    s0.5977    2.7588      0.2140  
00280   0.148    279.0748   0.6022    2.7585      0.2147     278.3658    s0.4364    2.7602      0.1685  
00281   0.156    277.9981   0.4402    2.7563      0.1680     277.3586    s0.5516    2.7440      0.2045  
00282   0.148    277.2755   0.5560    2.7429      0.2051     276.6740    s0.4844    2.7406      0.1877  
00283   0.147    276.2353   0.4884    2.7361      0.1876     276.1362    s0.4962    2.7346      0.1965  
00284   0.147    275.7415   0.5003    2.7304      0.1967     276.0018    s0.5589    2.7299      0.2152  
00285   0.146    275.7795   0.5635    2.7275      0.2162     276.4168    s0.4539    2.7396      0.1840  
00286   0.148    275.8246   0.4577    2.7335      0.1839     276.6047    s0.6021    2.7337      0.2276  
00287   0.148    276.4685   0.6070    2.7320      0.2294     275.5726    s0.4435    2.7318      0.1801  
00288   0.146    275.0154   0.4473    2.7260      0.1802     274.1839    s0.5249    2.7135      0.2112  
00289   0.147    273.8685   0.5294    2.7101      0.2126     273.8577    s0.5284    2.7099      0.2219  
00290   0.147    273.3195   0.5328    2.7043      0.2235     274.1316    s0.4844    2.7150      0.2126  
00291   0.149    273.4470   0.4884    2.7079      0.2141     274.2298    s0.5920    2.7103      0.2394  
00292   0.146    274.1089   0.5971    2.7088      0.2423     274.2196    s0.4404    2.7183      0.1922  
00293   0.146    273.5929   0.4446    2.7118      0.1928     273.1124    s0.5462    2.7015      0.2281  
00294   0.146    273.0237   0.5513    2.7004      0.2311     272.4344    s0.5097    2.6965      0.2308  
00295   0.147    271.8143   0.5144    2.6901      0.2329     271.7368    s0.5194    2.6891      0.2323  
00296   0.147    271.2855   0.5244    2.6843      0.2353     271.3582    s0.5306    2.6848      0.2285  
00297   0.147    271.2371   0.5360    2.6833      0.2316     272.1573    s0.4685    2.6959      0.2213  
00298   0.147    271.6023   0.4735    2.6901      0.2221     272.6959    s0.5882    2.6951      0.2477  
00299   0.147    272.7637   0.5937    2.6954      0.2513     273.4682    s0.4461    2.7101      0.2285  
00300   0.150    272.5240   0.4509    2.7004      0.2299     273.8594    s0.6348    2.7042      0.2677  
resampling
00301   0.147    274.4505   0.6414    2.7098      0.2660     272.1406    s0.4560    2.6964      0.2255  
00302   0.148    271.0490   0.4637    2.6851      0.2232     270.3531    s0.5094    2.6758      0.2212  
00303   0.147    269.6816   0.5178    2.6687      0.2206     269.9962    s0.5512    2.6699      0.2469  
00304   0.146    269.1658   0.5598    2.6612      0.2465     271.2360    s0.4763    2.6862      0.2356  
00305   0.148    269.6632   0.4850    2.6700      0.2355     271.4221    s0.5552    2.6842      0.2275  
00306   0.147    271.4321   0.5631    2.6839      0.2273     270.2389    s0.4574    2.6774      0.2171  
00307   0.156    269.0418   0.4658    2.6650      0.2157     268.9663    s0.5272    2.6610      0.2315  
00308   0.147    268.0054   0.5356    2.6510      0.2306     269.4907    s0.5302    2.6662      0.2228  
00309   0.147    268.3645   0.5391    2.6545      0.2240     270.7487    s0.4475    2.6830      0.2125  
00310   0.146    268.6745   0.4562    2.6618      0.2129     270.1179    s0.5928    2.6693      0.2284  
00311   0.147    270.1027   0.5997    2.6688      0.2275     269.0633    s0.4698    2.6651      0.2040  
00312   0.158    267.8862   0.4779    2.6529      0.2037     268.9777    s0.4674    2.6645      0.1916  
00313   0.146    267.3002   0.4762    2.6473      0.1936     269.0448    s0.5848    2.6589      0.2341  
00314   0.146    268.1301   0.5933    2.6493      0.2358     270.8965    s0.4702    2.6833      0.2136  
00315   0.147    268.6238   0.4787    2.6602      0.2149     271.0427    s0.5682    2.6800      0.2008  
00316   0.151    271.0146   0.5757    2.6793      0.2031     267.7060    s0.4704    2.6517      0.1863  
00317   0.147    266.5653   0.4792    2.6398      0.1883     270.9663    s0.5058    2.6821      0.2264  
00318   0.146    268.9631   0.5143    2.6616      0.2277     274.0102    s0.6441    2.7055      0.2360  
00319   0.146    273.7294   0.6521    2.7023      0.2399     271.5632    s0.4322    2.6924      0.1592  
00320   0.146    269.3225   0.4408    2.6696      0.1620     273.6691    s0.5834    2.7054      0.2122  
00321   0.146    272.8697   0.5890    2.6971      0.2109     272.6089    s0.6089    2.6936      0.1995  
00322   0.148    273.0052   0.6138    2.6974      0.2002     269.2821    s0.4321    2.6697      0.1514  
00323   0.153    267.9511   0.4405    2.6559      0.1539     273.6688    s0.4218    2.7140      0.1629  
00324   0.147    270.3121   0.4294    2.6800      0.1654     278.6657    s0.7507    2.7466      0.2522  
00325   0.146    278.7868   0.7574    2.7474      0.2552     267.6609    s0.5675    2.6464      0.1824  
resampling
00326   0.149    267.8267   0.5699    2.6479      0.1846     281.6105    s0.3241    2.7984      0.1470  
00327   0.148    281.5118   0.3289    2.7971      0.1524     274.0066    s0.5853    2.7091      0.1724  
00328   0.148    274.4664   0.5875    2.7135      0.1743     276.2638    s0.6666    2.7273      0.1963  
00329   0.148    276.6154   0.6685    2.7307      0.1983     270.3721    s0.4784    2.6782      0.1645  
00330   0.148    270.4233   0.4818    2.6785      0.1677     271.0435    s0.3918    2.6895      0.1325  
00331   0.148    270.9262   0.3961    2.6881      0.1374     272.8211    s0.6158    2.6956      0.1840  
00332   0.148    273.1011   0.6183    2.6982      0.1865     270.3459    s0.6080    2.6713      0.1733  
00333   0.146    270.4940   0.6103    2.6727      0.1751     271.1242    s0.4513    2.6871      0.1544  
00334   0.148    270.8816   0.4547    2.6845      0.1574     267.4964    s0.4206    2.6526      0.1374  
00335   0.147    267.1088   0.4242    2.6485      0.1414     270.1481    s0.5965    2.6698      0.1862  
00336   0.147    270.0036   0.5994    2.6682      0.1894     267.6342    s0.6000    2.6445      0.1849  
00337   0.147    267.4499   0.6026    2.6425      0.1874     269.0199    s0.4794    2.6646      0.1619  
00338   0.147    268.7224   0.4829    2.6614      0.1651     265.3651    s0.4811    2.6281      0.1538  
00339   0.155    265.2189   0.4842    2.6264      0.1561     267.6328    s0.5654    2.6463      0.1710  
00340   0.147    267.5591   0.5680    2.6455      0.1728     265.7411    s0.5316    2.6292      0.1621  
00341   0.147    265.5474   0.5344    2.6271      0.1642     266.7581    s0.4552    2.6433      0.1514  
00342   0.147    266.3774   0.4587    2.6393      0.1546     264.9538    s0.5333    2.6212      0.1715  
00343   0.147    264.6303   0.5360    2.6178      0.1736     265.6718    s0.5885    2.6255      0.1826  
00344   0.149    265.4066   0.5909    2.6227      0.1845     264.4348    s0.5138    2.6170      0.1616  
00345   0.147    264.1673   0.5170    2.6142      0.1641     265.1036    s0.4456    2.6273      0.1478  
00346   0.148    264.7525   0.4493    2.6235      0.1511     263.8983    s0.5311    2.6108      0.1657  
00347   0.147    263.4277   0.5335    2.6059      0.1678     264.5930    s0.5790    2.6152      0.1738  
00348   0.148    264.0653   0.5813    2.6098      0.1761     263.3234    s0.4993    2.6067      0.1557  
00349   0.146    262.6904   0.5020    2.6002      0.1588     264.0658    s0.4527    2.6165      0.1484  
00350   0.147    263.4737   0.4560    2.6104      0.1523     262.9391    s0.5444    2.6005      0.1698  
resampling
00351   0.150    260.9690   0.5485    2.5805      0.1725     262.7125    s0.5596    2.5974      0.1772  
00352   0.147    260.6957   0.5638    2.5770      0.1798     263.3999    s0.4801    2.6084      0.1613  
00353   0.148    260.7255   0.4834    2.5815      0.1630     262.7660    s0.5133    2.6003      0.1652  
00354   0.148    260.2476   0.5163    2.5750      0.1671     261.8481    s0.5425    2.5896      0.1728  
00355   0.147    259.6284   0.5467    2.5672      0.1752     263.0840    s0.5020    2.6041      0.1677  
00356   0.150    260.5752   0.5063    2.5787      0.1702     263.6202    s0.5273    2.6082      0.1681  
00357   0.147    261.2875   0.5306    2.5846      0.1708     261.5557    s0.5134    2.5882      0.1674  
00358   0.149    259.0945   0.5172    2.5634      0.1699     266.0736    s0.5155    2.6332      0.1738  
00359   0.157    263.2593   0.5204    2.6048      0.1766     266.5053    s0.5957    2.6334      0.1885  
00360   0.147    264.3005   0.5996    2.6111      0.1925     266.6920    s0.5431    2.6380      0.1757  
00361   0.146    264.3497   0.5457    2.6144      0.1787     262.5984    s0.5094    2.5988      0.1685  
00362   0.147    260.5465   0.5134    2.5781      0.1716     266.4849    s0.5492    2.6356      0.1782  
00363   0.147    264.6358   0.5546    2.6168      0.1818     263.2287    s0.5354    2.6038      0.1702  
00364   0.147    260.8816   0.5397    2.5801      0.1738     264.6282    s0.4990    2.6198      0.1555  
00365   0.147    262.1315   0.5017    2.5946      0.1581     263.3400    s0.5108    2.6062      0.1641  
00366   0.147    260.9355   0.5147    2.5819      0.1675     263.3436    s0.5133    2.6061      0.1700  
00367   0.146    260.7088   0.5173    2.5795      0.1728     262.5843    s0.5360    2.5973      0.1717  
00368   0.146    259.9329   0.5407    2.5705      0.1753     262.1678    s0.5390    2.5931      0.1597  
00369   0.148    259.8145   0.5431    2.5694      0.1635     261.5134    s0.4687    2.5902      0.1498  
00370   0.146    258.9560   0.4716    2.5644      0.1530     262.3359    s0.4726    2.5980      0.1703  
00371   0.147    259.4149   0.4756    2.5686      0.1738     261.3931    s0.5425    2.5849      0.1952  
00372   0.147    258.6097   0.5460    2.5568      0.1997     261.9220    s0.5463    2.5900      0.1875  
00373   0.162    259.1344   0.5490    2.5620      0.1909     261.1215    s0.5589    2.5814      0.1832  
00374   0.146    258.6886   0.5631    2.5569      0.1873     260.4969    s0.4970    2.5784      0.1696  
00375   0.146    258.0475   0.5003    2.5537      0.1732     261.2319    s0.4455    2.5884      0.1611  
resampling
00376   0.148    261.1931   0.4509    2.5877      0.1638     259.5583    s0.5218    2.5678      0.1652  
00377   0.147    259.5212   0.5245    2.5673      0.1656     260.2250    s0.5297    2.5742      0.1568  
00378   0.152    259.8886   0.5321    2.5707      0.1574     259.6075    s0.4978    2.5696      0.1578  
00379   0.149    258.9573   0.5010    2.5629      0.1599     259.9700    s0.4601    2.5752      0.1512  
00380   0.147    259.1170   0.4645    2.5664      0.1546     258.9593    s0.4883    2.5637      0.1494  
00381   0.147    258.3472   0.4919    2.5574      0.1515     259.5666    s0.5474    2.5667      0.1638  
00382   0.147    259.0907   0.5496    2.5618      0.1651     259.4814    s0.4911    2.5688      0.1439  
00383   0.146    258.6086   0.4944    2.5599      0.1452     260.8546    s0.5097    2.5813      0.1720  
00384   0.145    259.8177   0.5134    2.5708      0.1753     261.0312    s0.4586    2.5859      0.1456  
00385   0.147    259.7926   0.4632    2.5733      0.1478     261.6808    s0.5901    2.5854      0.1896  
00386   0.145    261.2017   0.5921    2.5805      0.1911     259.8522    s0.5142    2.5713      0.1518  
00387   0.146    259.2714   0.5179    2.5653      0.1523     258.6408    s0.4991    2.5598      0.1637  
00388   0.146    257.7249   0.5034    2.5504      0.1659     258.8493    s0.4562    2.5641      0.1583  
00389   0.148    257.6188   0.4615    2.5515      0.1614     258.0443    s0.5034    2.5537      0.1623  
00390   0.151    257.2009   0.5076    2.5450      0.1637     258.9182    s0.5903    2.5579      0.1771  
00391   0.146    258.3724   0.5925    2.5523      0.1770     257.5242    s0.5019    2.5486      0.1571  
00392   0.146    256.6588   0.5059    2.5397      0.1579     258.2293    s0.4519    2.5581      0.1558  
00393   0.148    257.0831   0.4568    2.5464      0.1581     257.8590    s0.5042    2.5517      0.1630  
00394   0.162    256.9416   0.5082    2.5424      0.1646     257.9217    s0.5151    2.5520      0.1495  
00395   0.148    257.2180   0.5191    2.5447      0.1495     258.9762    s0.5431    2.5609      0.1720  
00396   0.146    258.2577   0.5454    2.5536      0.1723     257.2987    s0.4860    2.5472      0.1536  
00397   0.146    256.1043   0.4900    2.5350      0.1549     258.9004    s0.4584    2.5647      0.1397  
00398   0.155    257.7387   0.4632    2.5528      0.1407     260.9396    s0.5412    2.5806      0.1739  
00399   0.148    260.3855   0.5434    2.5749      0.1739     258.0672    s0.5170    2.5532      0.1625  
00400   0.152    257.2155   0.5196    2.5445      0.1624     263.8207    s0.5121    2.6111      0.1471  
resampling
00401   0.153    262.8558   0.5159    2.6013      0.1477     256.9624    s0.5122    2.5424      0.1619  
00402   0.149    255.7795   0.5161    2.5303      0.1642     259.6844    s0.5367    2.5682      0.1810  
00403   0.147    258.8032   0.5407    2.5592      0.1835     260.8375    s0.5361    2.5797      0.1857  
00404   0.147    259.7503   0.5403    2.5686      0.1886     257.8278    s0.5184    2.5506      0.1768  
00405   0.152    256.3958   0.5228    2.5360      0.1802     258.7025    s0.5234    2.5592      0.1655  
00406   0.147    257.1103   0.5272    2.5431      0.1677     259.5342    s0.5418    2.5666      0.1616  
00407   0.146    258.1963   0.5460    2.5530      0.1633     256.6235    s0